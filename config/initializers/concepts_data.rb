# frozen_string_literal: true

MODEL_CONCEPTS = [
  { model: "R√©gression Lin√©aire",
    icon: "üìà",
    color: "blue",
    params_name: "Coefficients (poids + biais)",
    params_example: "prix = <strong>2847</strong> √ó surface + <strong>15200</strong> √ó nb_pi√®ces ‚àí <strong>8400</strong>",
    params_count: "1 par feature + 1 intercept",
    what_fit_does: "Minimise la somme des erreurs¬≤ (moindres carr√©s) pour trouver les meilleurs coefficients.",
    what_predict_does: "Multiplie chaque feature par son coefficient, additionne, renvoie le r√©sultat.",
    saved_model: "Un vecteur de N+1 nombres (coefficients + intercept).",
    code_inspect: "model.coef_, model.intercept_" },

  { model: "R√©gression Logistique",
    icon: "üéØ",
    color: "green",
    params_name: "Coefficients + sigmo√Øde",
    params_example: "P(spam) = œÉ(<strong>0.8</strong> √ó nb_liens + <strong>1.2</strong> √ó mots_suspects ‚àí <strong>3.1</strong>)",
    params_count: "1 par feature + 1 intercept",
    what_fit_does: "Maximise la vraisemblance (log-loss) pour s√©parer les classes.",
    what_predict_does: "Calcule la somme pond√©r√©e, passe dans la sigmo√Øde œÉ(x) = 1/(1+e‚ÅªÀ£), renvoie une probabilit√©.",
    saved_model: "Les m√™mes coefficients qu'une r√©gression lin√©aire. La sigmo√Øde est juste une transformation appliqu√©e dessus.",
    code_inspect: "model.coef_, model.intercept_" },

  { model: "Arbre de D√©cision",
    icon: "üå≥",
    color: "green",
    params_name: "Seuils de split + valeurs aux feuilles",
    params_example: "SI surface > <strong>80</strong> ET ville = <strong>Paris</strong> ‚Üí <strong>350 000‚Ç¨</strong>",
    params_count: "2 par n≈ìud (feature + seuil) + 1 par feuille",
    what_fit_does: "Teste tous les splits possibles, choisit celui qui r√©duit le plus l'impuret√© (Gini ou entropie).",
    what_predict_does: "Descend l'arbre en suivant les r√®gles, renvoie la valeur/classe de la feuille atteinte.",
    saved_model: "Une structure d'arbre : chaque n≈ìud stocke (quelle feature, quel seuil, enfant gauche, enfant droit).",
    code_inspect: "model.tree_.feature, model.tree_.threshold" },

  { model: "Random Forest",
    icon: "üå≤",
    color: "green",
    params_name: "N arbres ind√©pendants",
    params_example: "<strong>100 arbres</strong>, chacun entra√Æn√© sur un sous-√©chantillon al√©atoire, qui votent ensemble",
    params_count: "100 √ó (param√®tres d'un arbre) = des milliers de seuils",
    what_fit_does: "Entra√Æne N arbres en parall√®le sur des √©chantillons bootstrap (bagging). Chaque arbre ne voit qu'un sous-ensemble de features.",
    what_predict_does: "Fait pr√©dire chaque arbre, puis vote majoritaire (classif) ou moyenne (r√©gression).",
    saved_model: "Une liste de N arbres. Plus N est grand, plus le fichier est lourd.",
    code_inspect: "model.estimators_, model.feature_importances_" },

  { model: "XGBoost / Gradient Boosting",
    icon: "üöÄ",
    color: "yellow",
    params_name: "N arbres s√©quentiels + learning rate",
    params_example: "Arbre 1 pr√©dit 200K, erreur = +50K ‚Üí Arbre 2 corrige de <strong>‚àí50K √ó 0.1</strong> = ‚àí5K ‚Üí ...",
    params_count: "N arbres √ó param√®tres par arbre + hyperparam√®tres (lr, max_depth...)",
    what_fit_does: "Entra√Æne les arbres un par un. Chaque nouvel arbre apprend √† corriger les r√©sidus (erreurs) du pr√©c√©dent.",
    what_predict_does: "Somme les pr√©dictions de tous les arbres, pond√©r√©es par le learning rate.",
    saved_model: "Une s√©quence ordonn√©e d'arbres. L'ordre compte (contrairement au Random Forest).",
    code_inspect: "model.feature_importances_, model.get_booster()" },

  { model: "KNN (K-Nearest Neighbors)",
    icon: "üîµ",
    color: "cyan",
    params_name: "Aucun param√®tre appris !",
    params_example: "Le 'mod√®le' = <strong>le dataset d'entra√Ænement entier</strong>, stock√© en m√©moire",
    params_count: "0 param√®tres. Le dataset EST le mod√®le.",
    what_fit_does: "Stocke les donn√©es en m√©moire. C'est tout. (d'o√π le nom 'lazy learner')",
    what_predict_does: "Calcule la distance avec TOUS les points d'entra√Ænement, prend les K plus proches, vote.",
    saved_model: "Le dataset complet. Un KNN sur 1M de lignes = un fichier de 1M de lignes.",
    code_inspect: "model._fit_X (les donn√©es stock√©es)" },

  { model: "SVM (Support Vector Machine)",
    icon: "‚¨õ",
    color: "purple",
    params_name: "Vecteurs supports + poids",
    params_example: "Trouve la fronti√®re qui <strong>maximise la marge</strong> entre les classes. Ne retient que les points proches de la fronti√®re.",
    params_count: "Un sous-ensemble des donn√©es (les support vectors) + leurs poids",
    what_fit_does: "Cherche l'hyperplan qui s√©pare les classes avec la plus grande marge. Avec un kernel RBF, projette d'abord dans un espace de dimension sup√©rieure.",
    what_predict_does: "Calcule de quel c√¥t√© de la fronti√®re tombe le nouveau point.",
    saved_model: "Les vecteurs supports (quelques % du dataset) + les coefficients duaux.",
    code_inspect: "model.support_vectors_, model.dual_coef_" },

  { model: "K-Means",
    icon: "üß©",
    color: "purple",
    params_name: "Coordonn√©es des centro√Ødes",
    params_example: "Cluster 0 : centro√Øde √† (<strong>25 ans, 35K‚Ç¨, Paris</strong>), Cluster 1 : (<strong>45 ans, 72K‚Ç¨, Lyon</strong>)",
    params_count: "K √ó nombre de features",
    what_fit_does: "Place K centro√Ødes al√©atoirement, puis alterne : (1) assigne chaque point au centro√Øde le plus proche, (2) recalcule les centro√Ødes. R√©p√®te jusqu'√† convergence.",
    what_predict_does: "Calcule la distance avec chaque centro√Øde, renvoie le num√©ro du plus proche.",
    saved_model: "K vecteurs de dimension N (les centro√Ødes). Pour K=5 et 10 features : 50 nombres.",
    code_inspect: "model.cluster_centers_, model.labels_" },

  { model: "PCA",
    icon: "üßÆ",
    color: "purple",
    params_name: "Axes de projection (composantes principales)",
    params_example: "PC1 = <strong>0.7</strong> √ó surface + <strong>0.5</strong> √ó prix ‚àí <strong>0.1</strong> √ó √©tage (l'axe qui capte le plus de variance)",
    params_count: "n_components √ó n_features (une matrice de rotation)",
    what_fit_does: "Calcule les directions de variance maximale dans les donn√©es (vecteurs propres de la matrice de covariance).",
    what_predict_does: "Projette les donn√©es sur ces axes (transform, pas predict).",
    saved_model: "La matrice de composantes + la moyenne de chaque feature (pour centrer).",
    code_inspect: "pca.components_, pca.explained_variance_ratio_" },

  { model: "R√©seau de Neurones (Dense)",
    icon: "üß†",
    color: "red",
    params_name: "Matrices de poids + vecteurs de biais",
    params_example: "Couche Dense(64) avec 10 inputs : matrice <strong>10√ó64</strong> = 640 poids + <strong>64</strong> biais = <strong>704 param√®tres</strong>",
    params_count: "Peut aller de quelques centaines √† des milliards (GPT-4 ‚âà 1 700 milliards)",
    what_fit_does: "Propagation avant ‚Üí calcul de la loss ‚Üí r√©tropropagation du gradient ‚Üí mise √† jour des poids (gradient descent). R√©p√©t√© sur chaque batch, chaque epoch.",
    what_predict_does: "Propagation avant uniquement : multiplie les inputs par les poids, ajoute les biais, applique l'activation, couche par couche.",
    saved_model: "Toutes les matrices de poids de chaque couche. Un r√©seau simple ‚âà quelques Ko. VGG16 ‚âà 528 Mo.",
    code_inspect: "model.get_weights(), model.summary()" },
]
