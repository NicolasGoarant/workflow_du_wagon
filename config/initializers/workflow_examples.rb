# frozen_string_literal: true

WORKFLOW_EXAMPLES = {
  "preprocessing" => {
    title: "üè† Dataset immobilier ‚Äî 5 appartements √† nettoyer",
    dataframe: [
      ["surface", "pieces", "ville", "etage", "prix"],
      [65, 3, "Paris", 2, 285_000],
      [42, 2, "Lyon", nil, 158_000],
      [120, 5, "Paris", 4, nil],
      [65, 3, "Lyon", 0, 172_000],
      [42, 2, "Paris", 2, 285_000],
    ],
    problems: [
      "Ligne 2 : <code>etage</code> manquant (NaN) ‚Üí imputer par la m√©diane (2.0)",
      "Ligne 3 : <code>prix</code> manquant ‚Üí supprimer la ligne (c'est la target !)",
      "Lignes 1 et 5 : doublon exact ‚Üí <code>drop_duplicates()</code>",
      "<code>ville</code> est cat√©goriel ‚Üí One-Hot Encoding (ville_Paris, ville_Lyon)",
    ],
    result: [
      ["surface", "pieces", "etage", "ville_Lyon", "ville_Paris", "prix"],
      [65, 3, 2, 0, 1, 285_000],
      [42, 2, 2, 1, 0, 158_000],
      [65, 3, 0, 1, 0, 172_000],
    ],
    conclusion: "De 5 lignes √ó 5 colonnes √† 3 lignes √ó 6 colonnes. Propre, num√©rique, pr√™t pour le mod√®le.",
  },

  "eda" => {
    title: "üè† Exploration du dataset immobilier",
    dataframe: [
      ["surface", "pieces", "prix", "parking"],
      [35, 1, 125_000, 0],
      [65, 3, 285_000, 1],
      [42, 2, 158_000, 0],
      [110, 5, 520_000, 1],
      [78, 3, 310_000, 1],
    ],
    problems: [
      "<code>df.describe()</code> : surface de 35 √† 110, prix de 125K √† 520K ‚Äî pas d'outlier flagrant",
      "<code>df.corr()</code> : corr√©lation surface‚Üîprix = <strong>0.99</strong> (quasi parfaite, logique)",
      "Corr√©lation pieces‚Üîsurface = <strong>0.95</strong> ‚Üí multicolin√©arit√©, peut poser probl√®me en r√©gression",
      "Distribution de prix : skewed √† droite ‚Üí <code>np.log1p(prix)</code> pour normaliser",
    ],
    result: nil,
    conclusion: "L'EDA r√©v√®le que la surface est le meilleur pr√©dicteur du prix, et qu'il y a un risque de multicolin√©arit√© pieces/surface.",
  },

  "linreg" => {
    title: "üè† Pr√©dire le prix d'un appartement",
    dataframe: [
      ["surface", "pieces", "prix"],
      [35, 1, 125_000],
      [65, 3, 285_000],
      [42, 2, 158_000],
      [110, 5, 520_000],
    ],
    problems: [
      "Le mod√®le apprend : <code>prix = 4 890 √ó surface + 12 300 √ó pieces ‚àí 52 100</code>",
      "Coefficients : chaque m¬≤ suppl√©mentaire ‚Üí +4 890‚Ç¨, chaque pi√®ce ‚Üí +12 300‚Ç¨",
      "R¬≤ = 0.98 ‚Üí le mod√®le explique 98% de la variance du prix",
    ],
    result: [
      ["surface", "pieces", "prix_r√©el", "prix_pr√©dit"],
      [78, 3, "?", "~330 720‚Ç¨"],
    ],
    conclusion: "Le mod√®le pr√©dit ~330K‚Ç¨ pour 78m¬≤, 3 pi√®ces. RMSE = ~15 000‚Ç¨ ‚Üí erreur moyenne de 15K‚Ç¨.",
  },

  "logreg" => {
    title: "üìß D√©tecter les spams dans une bo√Æte mail",
    dataframe: [
      ["nb_liens", "mots_suspects", "longueur", "spam"],
      [0, 1, 450, 0],
      [8, 12, 120, 1],
      [1, 0, 800, 0],
      [15, 8, 95, 1],
      [2, 3, 350, 0],
    ],
    problems: [
      "Le mod√®le apprend : <code>P(spam) = œÉ(0.4 √ó nb_liens + 0.6 √ó mots_suspects ‚àí 0.01 √ó longueur ‚àí 2.1)</code>",
      "La sigmo√Øde œÉ transforme le score en probabilit√© [0, 1]",
      "Seuil par d√©faut = 0.5 : si P(spam) > 0.5 ‚Üí class√© spam",
    ],
    result: [
      ["nb_liens", "mots_suspects", "longueur", "P(spam)", "class√©"],
      [5, 6, 200, "0.87", "üî¥ Spam"],
    ],
    conclusion: "Nouvel email avec 5 liens et 6 mots suspects ‚Üí P(spam) = 87% ‚Üí class√© spam. On pourrait baisser le seuil √† 0.3 pour ne rien rater.",
  },

  "trees" => {
    title: "üéì Pr√©dire si un √©tudiant r√©ussit l'examen",
    dataframe: [
      ["heures_√©tude", "cours_suivis", "exercices_faits", "r√©ussite"],
      [2, 3, 5, 0],
      [8, 10, 20, 1],
      [5, 7, 12, 1],
      [1, 2, 3, 0],
      [6, 8, 15, 1],
    ],
    problems: [
      "L'arbre apprend des r√®gles : <strong>SI heures_√©tude > 4 ET exercices > 10 ‚Üí r√©ussite</strong>",
      "Pas besoin de scaling (seuls les seuils comptent)",
      "Random Forest : 100 arbres votent ‚Üí plus robuste qu'un seul arbre",
      "<code>feature_importances_</code> : exercices (0.52) > heures (0.31) > cours (0.17)",
    ],
    result: [
      ["heures_√©tude", "cours_suivis", "exercices_faits", "pr√©diction"],
      [4, 6, 11, "‚úÖ R√©ussit (78 arbres sur 100 votent oui)"],
    ],
    conclusion: "Le nombre d'exercices faits est le facteur n¬∞1. L'arbre le montre clairement ‚Äî plus interpr√©table qu'un r√©seau de neurones.",
  },

  "boosting" => {
    title: "üè† Prix immobilier ‚Äî pousser la performance",
    dataframe: [
      ["surface", "pieces", "etage", "parking", "prix"],
      [35, 1, 5, 0, 125_000],
      [65, 3, 2, 1, 285_000],
      [42, 2, 0, 0, 158_000],
      [110, 5, 4, 1, 520_000],
    ],
    problems: [
      "Arbre 1 pr√©dit tout √† 272K (moyenne). Erreurs : ‚àí147K, +13K, ‚àí114K, +248K",
      "Arbre 2 se concentre sur les grosses erreurs ‚Üí corrige de lr √ó erreur",
      "Avec lr=0.1 et 500 arbres ‚Üí chaque arbre fait un petit pas correctif",
      "Early stopping : arr√™te √† l'arbre 342 (la val_loss remontait)",
    ],
    result: [
      ["surface", "pieces", "etage", "parking", "prix_pr√©dit"],
      [78, 3, 3, 1, "~318 500‚Ç¨"],
    ],
    conclusion: "XGBoost : RMSE = 8 200‚Ç¨ vs 15 000‚Ç¨ pour la r√©gression lin√©aire. Le boosting gagne 45% d'erreur en moins.",
  },

  "knn" => {
    title: "üç∑ Classifier un vin (rouge / blanc / ros√©)",
    dataframe: [
      ["acidit√©", "sucre", "alcool", "type"],
      [7.4, 1.9, 11.5, "rouge"],
      [6.8, 5.2, 10.0, "blanc"],
      [7.1, 4.8, 9.5, "blanc"],
      [7.5, 1.5, 12.0, "rouge"],
      [6.9, 3.8, 11.0, "ros√©"],
    ],
    problems: [
      "‚ö†Ô∏è Scaling obligatoire : acidit√© [6.8‚Äì7.5] vs sucre [1.5‚Äì5.2] ‚Üí StandardScaler",
      "K=3 : pour un nouveau vin, on mesure la distance avec TOUS les vins connus",
      "On prend les 3 plus proches et on vote la majorit√©",
      "Le 'mod√®le' = le dataset entier stock√© en m√©moire (lazy learner)",
    ],
    result: [
      ["acidit√©", "sucre", "alcool", "3 voisins", "pr√©diction"],
      [7.0, 4.0, 10.5, "blanc, blanc, ros√©", "üç∑ Blanc (2 votes sur 3)"],
    ],
    conclusion: "Simple et efficace. Mais avec 1M de vins, chaque pr√©diction recalcule 1M de distances ‚Üí lent.",
  },

  "svm" => {
    title: "üè• Diagnostic tumeur (b√©nigne / maligne)",
    dataframe: [
      ["taille_noyau", "texture", "p√©rim√®tre", "diagnostic"],
      [13.5, 14.2, 87, "b√©nigne"],
      [20.1, 23.5, 132, "maligne"],
      [12.4, 15.7, 82, "b√©nigne"],
      [18.2, 21.0, 120, "maligne"],
    ],
    problems: [
      "‚ö†Ô∏è Scaling obligatoire (bas√© sur des distances)",
      "SVM cherche la fronti√®re (hyperplan) qui <strong>maximise la marge</strong> entre les classes",
      "Kernel RBF : projette les donn√©es dans un espace sup√©rieur si pas lin√©airement s√©parables",
      "Ne retient que les <strong>support vectors</strong> (points proches de la fronti√®re)",
    ],
    result: [
      ["taille_noyau", "texture", "p√©rim√®tre", "pr√©diction"],
      [16.0, 19.0, 105, "‚ö†Ô∏è Maligne (marge √©troite ‚Üí incertitude)"],
    ],
    conclusion: "SVM excelle sur les petits datasets haute dimension. Ici 3 features, 4 lignes ‚Äî il trouve la fronti√®re.",
  },

  "kmeans" => {
    title: "üõí Segmenter les clients d'un e-commerce",
    dataframe: [
      ["√¢ge", "d√©pense_mois", "fr√©quence_achat"],
      [22, 45, 12],
      [55, 320, 3],
      [28, 60, 15],
      [48, 280, 4],
      [25, 50, 10],
    ],
    problems: [
      "‚ö†Ô∏è Scaling obligatoire (√¢ge [22‚Äì55] vs d√©pense [45‚Äì320])",
      "K-Means avec K=2 (choisi par m√©thode du coude)",
      "It√®re : place 2 centro√Ødes ‚Üí assigne ‚Üí recalcule ‚Üí jusqu'√† convergence",
      "Pas de labels ‚Äî c'est au Data Scientist d'interpr√©ter les clusters",
    ],
    result: [
      ["√¢ge", "d√©pense", "fr√©quence", "cluster", "interpr√©tation"],
      ["22, 28, 25", "45‚Äì60‚Ç¨", "10‚Äì15√ó", "Cluster 0", "üõçÔ∏è Jeunes actifs, petits achats fr√©quents"],
      ["55, 48", "280‚Äì320‚Ç¨", "3‚Äì4√ó", "Cluster 1", "üíé Seniors, gros achats rares"],
    ],
    conclusion: "Deux profils clients identifi√©s sans labels. Le marketing peut adapter ses campagnes par segment.",
  },

  "pca" => {
    title: "üìä Compresser 5 features en 2 composantes",
    dataframe: [
      ["surface", "pieces", "salles_bain", "balcon_m2", "etage"],
      [65, 3, 1, 5, 2],
      [120, 5, 2, 12, 4],
      [42, 2, 1, 3, 1],
      [95, 4, 2, 8, 3],
    ],
    problems: [
      "‚ö†Ô∏è Scaling obligatoire avant PCA",
      "5 features corr√©l√©es ‚Üí PCA trouve 2 axes qui captent 94% de la variance",
      "PC1 = 0.52√ósurface + 0.48√ópieces + 0.45√ósalles_bain + ... ‚Üí axe 'taille globale'",
      "PC2 = 0.70√óetage ‚àí 0.30√óbalcon + ... ‚Üí axe 'hauteur vs ext√©rieur'",
    ],
    result: [
      ["PC1 (taille)", "PC2 (hauteur)", "variance capt√©e"],
      ["-0.82", "0.31", "PC1 : 78%"],
      ["1.95", "0.67", "PC2 : 16%"],
      ["-1.54", "-0.43", "Total : 94%"],
      ["0.41", "-0.55", ""],
    ],
    conclusion: "De 5 dimensions √† 2, en ne perdant que 6% d'info. On peut maintenant visualiser les donn√©es en 2D.",
  },

  "nn" => {
    title: "üéµ Pr√©dire le genre musical d'un morceau",
    dataframe: [
      ["tempo", "√©nergie", "dansabilit√©", "acoustique", "genre"],
      [120, 0.85, 0.72, 0.10, "pop"],
      [140, 0.95, 0.60, 0.05, "rock"],
      [90, 0.30, 0.45, 0.85, "classique"],
      [128, 0.78, 0.88, 0.15, "√©lectro"],
    ],
    problems: [
      "Architecture : Dense(64, relu) ‚Üí Dropout(0.3) ‚Üí Dense(32, relu) ‚Üí Dense(4, softmax)",
      "4 inputs ‚Üí 64 neurones ‚Üí 32 neurones ‚Üí 4 classes = <strong>2 340 param√®tres</strong>",
      "Loss : <code>categorical_crossentropy</code> (multi-classes)",
      "Entra√Æn√© 50 epochs, early stopping √† l'epoch 38",
    ],
    result: [
      ["tempo", "√©nergie", "dansabilit√©", "acoustique", "P(pop)", "P(rock)", "P(classique)", "P(√©lectro)"],
      [125, 0.80, 0.75, 0.12, "0.62", "0.18", "0.03", "0.17"],
    ],
    conclusion: "Le r√©seau sort des probabilit√©s par classe. Le morceau est class√© 'pop' avec 62% de confiance.",
  },

  "cnn" => {
    title: "üê± Classifier des images : chat ou chien ?",
    dataframe: [
      ["image", "taille", "canaux", "label"],
      ["chat_01.jpg", "224√ó224", "RGB (3)", "chat"],
      ["chien_01.jpg", "224√ó224", "RGB (3)", "chien"],
      ["chat_02.jpg", "224√ó224", "RGB (3)", "chat"],
      ["chien_02.jpg", "224√ó224", "RGB (3)", "chien"],
    ],
    problems: [
      "Input : 224√ó224√ó3 = <strong>150 528 valeurs</strong> par image (pixels RGB normalis√©s /255)",
      "Transfer Learning : VGG16 pr√©-entra√Æn√© (base.trainable = False)",
      "On ajoute : Flatten ‚Üí Dense(128, relu) ‚Üí Dropout(0.5) ‚Üí Dense(1, sigmoid)",
      "Le CNN apprend : bords ‚Üí textures ‚Üí formes ‚Üí oreilles pointues = chat",
    ],
    result: [
      ["image", "P(chat)", "P(chien)", "pr√©diction"],
      ["test_01.jpg", "0.92", "0.08", "üê± Chat"],
    ],
    conclusion: "Avec seulement 100 images d'entra√Ænement, le transfer learning atteint 94% d'accuracy gr√¢ce aux features pr√©-apprises.",
  },

  "rnn" => {
    title: "üìà Pr√©dire la temp√©rature de demain",
    dataframe: [
      ["jour", "temp", "humidit√©", "vent"],
      ["Lun", 12.5, 65, 15],
      ["Mar", 13.0, 60, 12],
      ["Mer", 14.2, 55, 10],
      ["Jeu", 13.8, 58, 14],
      ["Ven", "?", "?", "?"],
    ],
    problems: [
      "S√©quence de 4 jours ‚Üí pr√©dire le 5e. Le LSTM retient le contexte temporel",
      "Input shape : (batch, 4 timesteps, 3 features)",
      "LSTM(64) ‚Üí les portes forget/input/output g√®rent la m√©moire long terme",
      "Un RNN classique oublierait le lundi ; le LSTM le retient si c'est pertinent",
    ],
    result: [
      ["jour", "temp_pr√©dite"],
      ["Ven", "~14.0¬∞C"],
    ],
    conclusion: "Le LSTM capte la tendance ascendante (12.5 ‚Üí 14.2) et pr√©dit ~14.0¬∞C. Plus la s√©quence est longue, plus le contexte est riche.",
  },

  "pipeline" => {
    title: "üîß Tout assembler dans un Pipeline",
    dataframe: [
      ["surface", "ville", "etage", "prix"],
      [65, "Paris", 2, 285_000],
      [42, "Lyon", 0, 158_000],
      [110, "Paris", 4, 520_000],
    ],
    problems: [
      "Num√©rique (surface, etage) : <code>SimpleImputer(median)</code> ‚Üí <code>StandardScaler()</code>",
      "Cat√©goriel (ville) : <code>SimpleImputer(most_frequent)</code> ‚Üí <code>OneHotEncoder()</code>",
      "<code>ColumnTransformer</code> applique le bon preprocessing √† chaque type",
      "<code>Pipeline([('preproc', ct), ('model', Ridge())])</code> ‚Üí un seul objet",
    ],
    result: [
      ["Code", ""],
      ["pipe.fit(X_train, y_train)", "Fit le scaler + le mod√®le en 1 ligne"],
      ["pipe.predict(X_new)", "Transforme + pr√©dit automatiquement"],
      ["joblib.dump(pipe, 'pipe.pkl')", "Sauvegarde tout (preprocessing + mod√®le)"],
    ],
    conclusion: "Un Pipeline garantit que le preprocessing est identique en train et en production. Z√©ro risque de data leakage.",
  },

  "mlops" => {
    title: "üöÄ Du notebook au serveur de production",
    dataframe: [
      ["√âtape", "Outil", "Ce que √ßa fait"],
      ["1. Entra√Æner", "MLflow", "Log les params, m√©triques, et le mod√®le"],
      ["2. Versionner", "MLflow Registry", "Tague le meilleur mod√®le 'Production'"],
      ["3. Packager", "Docker", "Cr√©e un conteneur avec toutes les d√©pendances"],
      ["4. D√©ployer", "API (FastAPI)", "Expose /predict en endpoint HTTP"],
      ["5. Automatiser", "Prefect", "R√©-entra√Æne automatiquement chaque semaine"],
    ],
    problems: [
      "<code>mlflow.log_param('lr', 0.1)</code> ‚Üí tra√ßabilit√© des exp√©riences",
      "<code>mlflow.sklearn.log_model(pipe, 'model')</code> ‚Üí mod√®le versionn√©",
      "Docker : <code>FROM python:3.10</code> ‚Üí m√™me environnement partout",
      "Prefect : <code>@flow</code> + <code>@task</code> ‚Üí orchestration automatis√©e",
    ],
    result: nil,
    conclusion: "Le mod√®le passe de ton notebook Jupyter √† une API que n'importe qui peut appeler avec un simple curl.",
  },
}
