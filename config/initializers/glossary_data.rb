# frozen_string_literal: true

GLOSSARY = [
  # ── PREPROCESSING ──
  { term: "DataFrame", category: "Preprocessing", definition: "Structure tabulaire de Pandas (lignes × colonnes). C'est l'objet central de la Data Science en Python — ton tableur programmable.",
    code: 'df = pd.read_csv("housing.csv")  # → DataFrame', workflow: "preprocessing" },
  { term: "NaN / Valeur manquante", category: "Preprocessing", definition: "Case vide dans le dataset. Peut être aléatoire (MCAR), dépendante d'une autre variable (MAR), ou liée à la variable elle-même (MNAR).",
    code: 'df.isnull().sum()  # compte les NaN par colonne', workflow: "preprocessing" },
  { term: "Imputation", category: "Preprocessing", definition: "Remplir les valeurs manquantes. Médiane pour les numériques (robuste aux outliers), mode pour les catégorielles.",
    code: 'SimpleImputer(strategy="median").fit_transform(X)', workflow: "preprocessing" },
  { term: "Outlier", category: "Preprocessing", definition: "Valeur aberrante qui s'écarte fortement de la distribution. Détectée par IQR (1.5×) ou Z-score (> 3σ).",
    code: 'df["col"].clip(lower, upper)  # capper les extrêmes', workflow: "preprocessing" },
  { term: "One-Hot Encoding", category: "Preprocessing", definition: "Transforme une variable catégorielle en N colonnes binaires (1 par catégorie). Évite d'imposer un ordre artificiel.",
    code: 'OneHotEncoder(handle_unknown="ignore")', workflow: "preprocessing" },
  { term: "Ordinal Encoding", category: "Preprocessing", definition: "Encode les catégories avec un ordre naturel en entiers : S=0, M=1, L=2, XL=3.",
    code: 'OrdinalEncoder(categories=[["S","M","L","XL"]])', workflow: "preprocessing" },
  { term: "Feature Engineering", category: "Preprocessing", definition: "Créer de nouvelles variables à partir des existantes. Souvent plus impactant qu'un modèle plus complexe.",
    code: 'df["price_per_m2"] = df["price"] / df["surface"]', workflow: "preprocessing" },
  { term: "StandardScaler", category: "Preprocessing", definition: "Transforme chaque feature en moyenne=0, std=1. Formule : (x − μ) / σ. Indispensable pour KNN, SVM, réseaux de neurones.",
    code: 'scaler.fit_transform(X_train)  # fit sur train uniquement !', workflow: "preprocessing" },
  { term: "MinMaxScaler", category: "Preprocessing", definition: "Borne les valeurs entre 0 et 1. Utile pour les images et les réseaux de neurones.",
    code: 'MinMaxScaler().fit_transform(X)', workflow: "preprocessing" },
  { term: "RobustScaler", category: "Preprocessing", definition: "Utilise la médiane et l'IQR au lieu de la moyenne/std. Résistant aux outliers.",
    code: 'RobustScaler().fit_transform(X)', workflow: "preprocessing" },
  { term: "Data Leakage", category: "Preprocessing", definition: "Quand le modèle 'triche' en ayant accès à de l'info du test set pendant l'entraînement. Cause n°1 : scaler AVANT le split.",
    code: '# ⚠️ TOUJOURS : split → fit(train) → transform(test)', workflow: "preprocessing" },
  { term: "Train/Test Split", category: "Preprocessing", definition: "Séparer le dataset en deux : 70-80% pour apprendre, 20-30% pour évaluer. Toujours AVANT le preprocessing.",
    code: 'train_test_split(X, y, test_size=0.3, random_state=42)', workflow: "preprocessing" },
  { term: "Doublons", category: "Preprocessing", definition: "Lignes identiques qui faussent la distribution. Le modèle surpondère ces profils.",
    code: 'df.drop_duplicates()', workflow: "preprocessing" },
  { term: "IQR (Inter-Quartile Range)", category: "Preprocessing", definition: "Écart entre le 25e et le 75e percentile. Sert à détecter les outliers : tout ce qui dépasse 1.5×IQR au-delà des quartiles.",
    code: 'IQR = Q3 - Q1; upper = Q3 + 1.5 * IQR', workflow: "preprocessing" },

  # ── EDA ──
  { term: "EDA (Exploratory Data Analysis)", category: "EDA", definition: "Exploration systématique du dataset avant la modélisation. Distributions, corrélations, outliers, patterns de NaN.",
    code: 'df.describe(); df.corr(); sns.pairplot(df)', workflow: "eda" },
  { term: "Corrélation", category: "EDA", definition: "Mesure de la relation linéaire entre deux variables (-1 à 1). Forte corrélation ≠ causalité.",
    code: 'df.corr()["price"].sort_values(ascending=False)', workflow: "eda" },
  { term: "Multicolinéarité", category: "EDA", definition: "Deux features très corrélées entre elles (> 0.8). Rend les coefficients de la régression linéaire instables.",
    code: 'variance_inflation_factor(X.values, i)  # VIF > 5 = problème', workflow: "eda" },
  { term: "Distribution skewed", category: "EDA", definition: "Distribution asymétrique avec une longue traîne (ex: les prix immobiliers). Le log la rend plus gaussienne.",
    code: 'df["price"].apply(np.log1p)  # log(1+x)', workflow: "eda" },
  { term: "Heatmap", category: "EDA", definition: "Carte thermique des corrélations. Permet de repérer en un coup d'œil les relations fortes et la multicolinéarité.",
    code: 'sns.heatmap(df.corr(), annot=True, cmap="coolwarm")', workflow: "eda" },
  { term: "MCAR / MAR / MNAR", category: "EDA", definition: "Trois types de données manquantes : totalement aléatoire, dépendant d'une autre variable, ou dépendant de la variable elle-même (le plus dangereux).",
    code: 'import missingno as msno; msno.matrix(df)', workflow: "eda" },

  # ── MÉTRIQUES ──
  { term: "R² (coefficient de détermination)", category: "Métriques", definition: "% de variance expliquée par le modèle. 0.85 = le modèle capte 85% du signal. 1.0 = parfait.",
    code: 'r2_score(y_test, y_pred)', workflow: "linreg" },
  { term: "RMSE", category: "Métriques", definition: "Racine de l'erreur quadratique moyenne. En unité de la target (€, kg...). Pénalise fortement les grosses erreurs.",
    code: 'np.sqrt(mean_squared_error(y_test, y_pred))', workflow: "linreg" },
  { term: "MAE", category: "Métriques", definition: "Erreur absolue moyenne. Plus robuste aux outliers que le RMSE.",
    code: 'mean_absolute_error(y_test, y_pred)', workflow: "linreg" },
  { term: "Accuracy", category: "Métriques", definition: "% de bonnes prédictions. Trompeuse si classes déséquilibrées (95% négatifs → 95% en prédisant toujours négatif).",
    code: 'accuracy_score(y_test, y_pred)', workflow: "logreg" },
  { term: "Precision", category: "Métriques", definition: "Parmi les prédits positifs, combien le sont vraiment ? Importante quand les faux positifs coûtent cher (spam → mail bloqué).",
    code: 'precision_score(y_test, y_pred)', workflow: "logreg" },
  { term: "Recall (Sensibilité)", category: "Métriques", definition: "Parmi les vrais positifs, combien sont détectés ? Important quand les faux négatifs coûtent cher (fraude non détectée).",
    code: 'recall_score(y_test, y_pred)', workflow: "logreg" },
  { term: "F1-Score", category: "Métriques", definition: "Moyenne harmonique de precision et recall. Bon compromis quand on veut optimiser les deux.",
    code: 'f1_score(y_test, y_pred)', workflow: "logreg" },
  { term: "AUC-ROC", category: "Métriques", definition: "Aire sous la courbe ROC. Mesure la performance à TOUS les seuils. 0.5 = hasard, > 0.9 = excellent.",
    code: 'roc_auc_score(y_test, y_proba)', workflow: "logreg" },
  { term: "Matrice de confusion", category: "Métriques", definition: "Tableau 2×2 : True/False × Positive/Negative. Montre les 4 types de prédictions (TP, TN, FP, FN).",
    code: 'confusion_matrix(y_test, y_pred)', workflow: "logreg" },
  { term: "Cross-Validation", category: "Métriques", definition: "Découper le train en K folds, entraîner K fois en laissant un fold de côté. Score moyen ± std → mesure de stabilité.",
    code: 'cross_val_score(model, X, y, cv=5, scoring="r2")', workflow: "linreg" },

  # ── MODÈLES SUPERVISÉS ──
  { term: "Régression Linéaire", category: "Modèles", definition: "Trouve la droite (hyperplan) qui minimise la somme des erreurs au carré. Simple, interprétable, baseline classique.",
    code: 'LinearRegression().fit(X_train, y_train)', workflow: "linreg" },
  { term: "Ridge (L2)", category: "Modèles", definition: "Régression linéaire + pénalité sur la somme des coefficients². Réduit tous les coefficients mais n'en supprime aucun.",
    code: 'Ridge(alpha=1.0).fit(X_train, y_train)', workflow: "linreg" },
  { term: "Lasso (L1)", category: "Modèles", definition: "Régression linéaire + pénalité sur la somme des |coefficients|. Met certains à zéro → sélection de features automatique.",
    code: 'Lasso(alpha=100).fit(X_train, y_train)', workflow: "linreg" },
  { term: "Régression Logistique", category: "Modèles", definition: "Classification binaire via une sigmoïde. Sort une probabilité [0,1]. Le seuil (default 0.5) détermine la classe.",
    code: 'LogisticRegression(C=1.0, class_weight="balanced")', workflow: "logreg" },
  { term: "Decision Tree", category: "Modèles", definition: "Pose des questions successives (surface > 80 ? ville = Paris ?) pour séparer les données. Interprétable mais overfitte seul.",
    code: 'DecisionTreeClassifier(max_depth=4)', workflow: "trees" },
  { term: "Random Forest", category: "Modèles", definition: "Ensemble de N arbres entraînés sur des sous-échantillons aléatoires. Le vote majoritaire est plus fiable qu'un arbre seul (bagging).",
    code: 'RandomForestClassifier(n_estimators=100, max_features="sqrt")', workflow: "trees" },
  { term: "Bagging", category: "Modèles", definition: "Entraîner N modèles indépendants sur des échantillons bootstrap, puis moyenner. Réduit la variance (Random Forest).",
    code: nil, workflow: "trees" },
  { term: "Gradient Boosting", category: "Modèles", definition: "Arbres séquentiels où chacun corrige les erreurs du précédent. Réduit le biais. XGBoost, LightGBM, CatBoost.",
    code: 'XGBClassifier(learning_rate=0.05, n_estimators=300)', workflow: "boosting" },
  { term: "XGBoost", category: "Modèles", definition: "Implémentation optimisée du Gradient Boosting. Gère les NaN nativement. Souvent le meilleur sur données tabulaires.",
    code: 'XGBClassifier(early_stopping_rounds=50)', workflow: "boosting" },
  { term: "KNN (K-Nearest Neighbors)", category: "Modèles", definition: "Prédit en regardant les K voisins les plus proches. Lazy learner (pas de vrai modèle). Scaling obligatoire.",
    code: 'KNeighborsClassifier(n_neighbors=5)', workflow: "knn" },
  { term: "SVM (Support Vector Machine)", category: "Modèles", definition: "Trouve la frontière de marge maximale entre les classes. Le kernel RBF gère les cas non-linéaires. Scaling obligatoire.",
    code: 'SVC(C=10, gamma="scale", kernel="rbf")', workflow: "svm" },
  { term: "Kernel RBF", category: "Modèles", definition: "Projette les données dans un espace de dimension supérieure pour les rendre linéairement séparables. Le 'truc' du SVM.",
    code: 'SVC(kernel="rbf")', workflow: "svm" },

  # ── HYPERPARAMÈTRES & TUNING ──
  { term: "Hyperparamètre", category: "Tuning", definition: "Paramètre fixé AVANT l'entraînement (max_depth, learning_rate, K...). Contrairement aux poids, il n'est pas appris.",
    code: nil, workflow: "trees" },
  { term: "GridSearchCV", category: "Tuning", definition: "Teste TOUTES les combinaisons d'hyperparamètres. Exhaustif mais lent. O(n₁ × n₂ × ... × nₖ × folds).",
    code: 'GridSearchCV(model, params, cv=5, scoring="f1")', workflow: "svm" },
  { term: "RandomizedSearchCV", category: "Tuning", definition: "Teste N combinaisons aléatoires. Beaucoup plus rapide que Grid, souvent aussi efficace.",
    code: 'RandomizedSearchCV(model, params, n_iter=50, cv=5)', workflow: "trees" },
  { term: "Early Stopping", category: "Tuning", definition: "Arrête l'entraînement quand la performance sur le validation set stagne. Évite l'overfitting et le gaspillage de temps.",
    code: 'EarlyStopping(patience=5, restore_best_weights=True)', workflow: "boosting" },
  { term: "Learning Rate", category: "Tuning", definition: "Pas d'apprentissage. Petit = prudent et lent. Grand = rapide mais instable. Règle : baisser lr ET augmenter n_estimators.",
    code: 'XGBClassifier(learning_rate=0.05)', workflow: "boosting" },
  { term: "Overfitting", category: "Tuning", definition: "Le modèle mémorise le train au lieu d'apprendre. Train score >> Test score. Solutions : régularisation, dropout, early stopping, plus de données.",
    code: nil, workflow: "linreg" },
  { term: "Underfitting", category: "Tuning", definition: "Le modèle est trop simple pour capturer le signal. Train et Test scores faibles. Solutions : modèle plus complexe, plus de features.",
    code: nil, workflow: "linreg" },
  { term: "class_weight='balanced'", category: "Tuning", definition: "Donne un poids inversement proportionnel à la fréquence de chaque classe. Compense le déséquilibre sans rééchantillonner.",
    code: 'LogisticRegression(class_weight="balanced")', workflow: "logreg" },

  # ── NON-SUPERVISÉ ──
  { term: "K-Means", category: "Non-supervisé", definition: "Regroupe les données en K clusters autour de centroïdes. Itératif : assigner → recalculer centroïde → répéter.",
    code: 'KMeans(n_clusters=4, n_init=10)', workflow: "kmeans" },
  { term: "Inertia", category: "Non-supervisé", definition: "Somme des distances² au centroïde. Diminue avec K. Le 'coude' indique le bon nombre de clusters.",
    code: 'km.inertia_', workflow: "kmeans" },
  { term: "Silhouette Score", category: "Non-supervisé", definition: "Mesure la qualité des clusters (-1 à 1). Proche de 1 = bien séparé. Proche de 0 = chevauchement.",
    code: 'silhouette_score(X_sc, labels)', workflow: "kmeans" },
  { term: "PCA (Principal Component Analysis)", category: "Non-supervisé", definition: "Réduit les dimensions en trouvant les axes de variance maximale. 50 features → 5 composantes captant 95% de l'info.",
    code: 'PCA(n_components=0.95).fit_transform(X_sc)', workflow: "pca" },
  { term: "Variance expliquée", category: "Non-supervisé", definition: "% d'information capté par chaque composante PCA. PC1 > PC2 > PC3... On cumule jusqu'à 95%.",
    code: 'pca.explained_variance_ratio_', workflow: "pca" },

  # ── DEEP LEARNING ──
  { term: "Neurone / Dense Layer", category: "Deep Learning", definition: "Unité de base : somme pondérée des entrées + biais + fonction d'activation. Un Dense layer = N neurones fully connected.",
    code: 'Dense(64, activation="relu")', workflow: "nn" },
  { term: "ReLU", category: "Deep Learning", definition: "Fonction d'activation : max(0, x). Simple, efficace, résout le vanishing gradient. Le défaut pour les couches cachées.",
    code: 'Dense(64, activation="relu")', workflow: "nn" },
  { term: "Sigmoid", category: "Deep Learning", definition: "Fonction d'activation : 1/(1+e⁻ˣ). Sort entre 0 et 1 → probabilité. Utilisée en dernière couche pour la classification binaire.",
    code: 'Dense(1, activation="sigmoid")', workflow: "nn" },
  { term: "Softmax", category: "Deep Learning", definition: "Généralisation de sigmoid pour N classes. Sort N probabilités qui somment à 1.",
    code: 'Dense(n_classes, activation="softmax")', workflow: "nn" },
  { term: "Dropout", category: "Deep Learning", definition: "Éteint aléatoirement X% des neurones à chaque batch. Force la redondance → anti-overfitting.",
    code: 'Dropout(0.3)  # 30% des neurones éteints', workflow: "nn" },
  { term: "Batch Normalization", category: "Deep Learning", definition: "Normalise les activations entre les couches. Stabilise et accélère l'entraînement.",
    code: 'BatchNormalization()', workflow: "nn" },
  { term: "Backpropagation", category: "Deep Learning", definition: "Algorithme qui calcule le gradient de la loss par rapport à chaque poids, couche par couche, de la sortie vers l'entrée.",
    code: nil, workflow: "nn" },
  { term: "Loss / Fonction de coût", category: "Deep Learning", definition: "Ce que le modèle minimise. Binary crossentropy (classif binaire), MSE (régression), categorical crossentropy (multi-classes).",
    code: 'model.compile(loss="binary_crossentropy")', workflow: "nn" },
  { term: "Adam (optimizer)", category: "Deep Learning", definition: "Optimiseur adaptatif qui ajuste le learning rate par paramètre. Le défaut pour commencer en deep learning.",
    code: 'model.compile(optimizer="adam")', workflow: "nn" },
  { term: "Epoch", category: "Deep Learning", definition: "Un passage complet sur tout le dataset d'entraînement. 100 epochs = le modèle a vu chaque observation 100 fois.",
    code: 'model.fit(X, y, epochs=100)', workflow: "nn" },
  { term: "Batch Size", category: "Deep Learning", definition: "Nombre d'observations traitées avant une mise à jour des poids. 32 est le défaut. Plus petit = plus de bruit, plus grand = plus stable.",
    code: 'model.fit(X, y, batch_size=32)', workflow: "nn" },
  { term: "Convolution (CNN)", category: "Deep Learning", definition: "Filtre glissant qui détecte des motifs locaux dans une image (bords, textures). Partage de poids → efficace.",
    code: 'Conv2D(32, (3,3), activation="relu")', workflow: "cnn" },
  { term: "Transfer Learning", category: "Deep Learning", definition: "Réutiliser un réseau pré-entraîné (VGG16, ResNet) et ne ré-entraîner que les dernières couches. Peu de données → excellents résultats.",
    code: 'VGG16(weights="imagenet", include_top=False)', workflow: "cnn" },
  { term: "Data Augmentation", category: "Deep Learning", definition: "Créer des variantes artificielles des images (rotation, flip, zoom) pour augmenter le dataset d'entraînement.",
    code: 'ImageDataGenerator(rotation_range=20, horizontal_flip=True)', workflow: "cnn" },
  { term: "Embedding", category: "Deep Learning", definition: "Transforme un entier (index de mot) en vecteur dense. Les mots proches en sens ont des vecteurs proches.",
    code: 'Embedding(vocab_size, 128, input_length=200)', workflow: "rnn" },
  { term: "LSTM", category: "Deep Learning", definition: "Long Short-Term Memory. RNN avec portes (oublier, retenir, sortir) qui résout le vanishing gradient sur les longues séquences.",
    code: 'LSTM(64)', workflow: "rnn" },
  { term: "Bidirectional", category: "Deep Learning", definition: "Lit la séquence à l'endroit ET à l'envers. Le contexte des deux côtés désambiguïse ('banque' = financière ou rivière ?).",
    code: 'Bidirectional(LSTM(64))', workflow: "rnn" },
  { term: "Tokenizer", category: "Deep Learning", definition: "Transforme du texte en séquences de nombres. Chaque mot → un index. Les rares deviennent <OOV>.",
    code: 'Tokenizer(num_words=10000, oov_token="<OOV>")', workflow: "rnn" },
  { term: "Padding", category: "Deep Learning", definition: "Uniformise la longueur des séquences en ajoutant des 0. Le réseau a besoin d'entrées de taille fixe.",
    code: 'pad_sequences(sequences, maxlen=200)', workflow: "rnn" },

  # ── PRODUCTION ──
  { term: "Pipeline (sklearn)", category: "Production", definition: "Enchaîne preprocessing + modèle dans un objet unique. fit_transform/transform géré automatiquement. Zéro data leakage.",
    code: 'Pipeline([("prep", preprocessor), ("model", rf)])', workflow: "pipeline" },
  { term: "ColumnTransformer", category: "Production", definition: "Applique des transformations différentes selon le type de colonne (numériques → scaler, catégorielles → encoder).",
    code: 'ColumnTransformer([("num", num_pipe, num_cols), ...])', workflow: "pipeline" },
  { term: "joblib", category: "Production", definition: "Sérialise un modèle/pipeline Python en fichier. Sauvegarde TOUT (scaler, encoder, modèle). Un seul load en production.",
    code: 'joblib.dump(pipe, "pipeline.joblib")', workflow: "pipeline" },
  { term: "FastAPI", category: "Production", definition: "Framework Python pour créer des API REST. Typage auto, docs Swagger générée, async natif.",
    code: '@app.get("/predict")\ndef predict(age: int): ...', workflow: "mlops" },
  { term: "Docker", category: "Production", definition: "Conteneur isolé avec ton environnement exact. Plus de 'ça marche sur ma machine'. Image reproductible.",
    code: 'FROM python:3.10-slim', workflow: "mlops" },
  { term: "Cloud Run", category: "Production", definition: "Service serverless GCP. 0 requêtes = 0€. Auto-scaling. HTTPS automatique. Déploiement en 1 commande.",
    code: 'gcloud run deploy --image gcr.io/PROJECT/model', workflow: "mlops" },
  { term: "API endpoint", category: "Production", definition: "URL qui accepte des requêtes HTTP et retourne des données (JSON). L'interface entre ton modèle et le monde extérieur.",
    code: 'curl "localhost:8000/predict?age=35&income=55000"', workflow: "mlops" },
]
