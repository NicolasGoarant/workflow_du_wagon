# frozen_string_literal: true
# All 15 Le Wagon DS workflows with detailed code, outputs, and annotations
# Using Ruby heredocs = zero indentation issues

WORKFLOWS = [
  {
    slug: "preprocessing",
    title: "Data Preprocessing",
    icon: "üßπ",
    badge: "Pr√©paration",
    badge_color: "green",
    subtitle: "Nettoyer et transformer les donn√©es brutes en un dataset exploitable par un mod√®le",
    analogy_title: "L'analogie du chef cuisinier",
    analogy_text: "Le preprocessing, c'est comme la mise en place en cuisine. Avant de cuisiner (entra√Æner un mod√®le), il faut laver les l√©gumes (supprimer les doublons), retirer les parties ab√Æm√©es (g√©rer les valeurs manquantes), tout couper √† la m√™me taille (scaling) et s√©parer les ingr√©dients par type (encodage). Si tu sautes cette √©tape, le plat sera rat√©, m√™me avec la meilleure recette.",
    steps: [
      {
        title: "Charger et inspecter les donn√©es",
        explain: "On charge le CSV et on prend la temp√©rature du dataset. Combien de lignes ? De colonnes ? Quels types ? Des valeurs aberrantes visibles d√®s le describe() ?",
        code_block: <<~PY,
          import pandas as pd

          df = pd.read_csv("housing.csv")
          print(df.shape)
          print(df.dtypes)
          print(df.describe())
          print(df.head(3))
        PY
        output: <<~OUT,
          (1460, 7)

          surface     int64
          rooms       int64
          age         int64
          city       object
          garden     object
          pool       object
          price       int64

               surface  rooms    age     price
          mean   102.3   3.8   22.1   245_800
          min     18.0   1.0    0.0    45_000
          max    450.0  12.0   95.0  1_200_000

             surface  rooms  age    city     garden  pool    price
          0       85      3   15    Paris     Oui    Non   320000
          1      120      5   30    Lyon      Non    Non   185000
          2       45      2    5    Marseille Oui    Oui   142000
        OUT
        code_notes: [
          { marker: "df.shape", text: "Donne (lignes, colonnes). Ici 1460 observations et 7 variables. C'est le premier r√©flexe." },
          { marker: "df.dtypes", text: "V√©rifie les types. <code>city</code> est <code>object</code> (texte) ‚Üí il faudra l'encoder. <code>price</code> est <code>int64</code> ‚Üí c'est notre target num√©rique." },
          { marker: "describe()", text: "R√©sum√© statistique. Un <code>age</code> min √† 0 est suspect (maison neuve ou erreur ?). Un <code>surface</code> max √† 450 m¬≤ est-il un outlier ?" },
        ]
      },
      {
        title: "G√©rer les valeurs manquantes",
        explain: "Des cases vides dans le dataset. On les d√©tecte, on comprend le pattern (al√©atoire ou syst√©matique ?), puis on les traite : suppression ou remplissage intelligent.",
        code_block: <<~PY,
          # Compter les NaN par colonne
          print(df.isnull().sum())
          print(f"\\n% manquants :\\n{(df.isnull().mean() * 100).round(1)}")

          # Strat√©gie selon le % de manquants
          # < 5%  ‚Üí remplir (imputer)
          # > 60% ‚Üí supprimer la colonne
          # Entre ‚Üí analyser le pattern

          from sklearn.impute import SimpleImputer

          # Num√©riques ‚Üí m√©diane (robuste aux outliers)
          imputer_num = SimpleImputer(strategy="median")
          df["surface"] = imputer_num.fit_transform(df[["surface"]])

          # Cat√©gorielles ‚Üí valeur la plus fr√©quente
          imputer_cat = SimpleImputer(strategy="most_frequent")
          df["city"] = imputer_cat.fit_transform(df[["city"]]).ravel()
        PY
        output: <<~OUT,
          surface     23
          rooms        0
          age         45
          city        12
          garden       8
          pool         0
          price        0

          % manquants :
          surface    1.6
          rooms      0.0
          age        3.1
          city       0.8
          garden     0.5
          pool       0.0
          price      0.0
        OUT
        code_notes: [
          { marker: "isnull().sum()", text: "Compte les NaN par colonne. Ici <code>age</code> a 45 valeurs manquantes (3.1 %). C'est peu ‚Üí on impute." },
          { marker: "strategy='median'", text: "Pourquoi la m√©diane et pas la moyenne ? Parce que la m√©diane est robuste aux outliers. Si tu as des surfaces de 18 √† 450 m¬≤, la moyenne sera tir√©e vers le haut." },
          { marker: "most_frequent", text: "Pour les cat√©gories, on remplit avec la valeur la plus courante. Si 60 % des lignes ont <code>city='Paris'</code>, les NaN deviennent <code>Paris</code>." },
        ]
      },
      {
        title: "Supprimer les doublons",
        explain: "Des lignes identiques qui faussent la distribution. Le mod√®le pensera que ce profil est plus fr√©quent qu'il ne l'est.",
        code_block: <<~PY,
          print(f"Doublons : {df.duplicated().sum()}")

          # Voir les doublons
          print(df[df.duplicated(keep=False)].sort_values("price").head(4))

          # Supprimer
          df = df.drop_duplicates()
          print(f"Shape apr√®s : {df.shape}")
        PY
        output: <<~OUT,
          Doublons : 17

             surface  rooms  age  city   garden pool   price
          42      85      3   15  Paris   Oui   Non  320000
          98      85      3   15  Paris   Oui   Non  320000
          67     120      5   30  Lyon    Non   Non  185000
          201    120      5   30  Lyon    Non   Non  185000

          Shape apr√®s : (1443, 7)
        OUT
        code_notes: [
          { marker: "duplicated()", text: "Renvoie <code>True</code> pour chaque ligne qui est une copie exacte d'une autre. <code>keep=False</code> marque TOUTES les copies (pas juste la 2e)." },
          { marker: "drop_duplicates()", text: "Supprime les doublons, garde la premi√®re occurrence. 17 lignes en moins ici." },
        ]
      },
      {
        title: "D√©tecter et traiter les outliers",
        explain: "Un √©l√®ve qui mesure '17 m√®tres' est clairement une erreur de saisie. Les outliers faussent la moyenne et le mod√®le ‚Äî on les rep√®re et on d√©cide quoi en faire.",
        code_block: <<~PY,
          import numpy as np

          # M√©thode IQR (Inter-Quartile Range)
          Q1 = df["surface"].quantile(0.25)   # 25e percentile
          Q3 = df["surface"].quantile(0.75)   # 75e percentile
          IQR = Q3 - Q1

          lower = Q1 - 1.5 * IQR
          upper = Q3 + 1.5 * IQR

          outliers = df[(df["surface"] < lower) | (df["surface"] > upper)]
          print(f"Bornes : [{lower:.0f}, {upper:.0f}]")
          print(f"Outliers surface : {len(outliers)}")

          # Option 1 : Supprimer
          df_clean = df[(df["surface"] >= lower) & (df["surface"] <= upper)]

          # Option 2 : Capper (remplacer par la borne)
          df["surface"] = df["surface"].clip(lower, upper)
        PY
        output: <<~OUT,
          Bornes : [12, 238]
          Outliers surface : 23

          Exemples d'outliers :
             surface  rooms  age   city      price
          88     450     12   10   Paris   1200000   ‚Üê ch√¢teau ?
          342      5      1   80   Marseille  45000  ‚Üê studio 5 m¬≤ ?
        OUT
        code_notes: [
          { marker: "IQR", text: "L'√©cart entre le 1er et 3e quartile. 50 % des donn√©es sont dans cet intervalle. On consid√®re comme outlier tout ce qui d√©passe 1.5√ó l'IQR au-del√†." },
          { marker: "clip()", text: "Au lieu de supprimer, on plafonne/plancher. Un 450 m¬≤ devient 238 m¬≤. Moins de perte de donn√©es, mais attention √† ne pas d√©former la distribution." },
        ]
      },
      {
        title: "Encoder les variables cat√©gorielles",
        explain: "Un mod√®le ML ne comprend que les chiffres. Il faut transformer 'Paris', 'Lyon', 'Marseille' en nombres ‚Äî mais intelligemment.",
        code_block: <<~PY,
          from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

          # CAS 1 ‚Äî Ordinales (il y a un ordre naturel)
          # Ex: taille = "S" < "M" < "L" < "XL"
          oe = OrdinalEncoder(categories=[["S", "M", "L", "XL"]])
          df["taille_encoded"] = oe.fit_transform(df[["taille"]])
          # S ‚Üí 0, M ‚Üí 1, L ‚Üí 2, XL ‚Üí 3

          # CAS 2 ‚Äî Nominales (pas d'ordre)
          # Ex: city = Paris, Lyon, Marseille
          ohe = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
          encoded = ohe.fit_transform(df[["city"]])
          print(ohe.get_feature_names_out())
          print(encoded[:3])
        PY
        output: <<~OUT,
          ['city_Lyon' 'city_Marseille' 'city_Nancy' 'city_Paris']

          [[0. 0. 0. 1.]     ‚Üê Paris
           [1. 0. 0. 0.]     ‚Üê Lyon
           [0. 1. 0. 0.]]    ‚Üê Marseille
        OUT
        code_notes: [
          { marker: "OrdinalEncoder", text: "Pour les variables avec un ORDRE. Taille S < M < L ‚Üí 0, 1, 2. Le mod√®le comprend que L > S." },
          { marker: "OneHotEncoder", text: "Pour les variables SANS ordre. Cr√©er <code>city=1, Lyon=2</code> impliquerait que Lyon > Paris. Le One-Hot cr√©e une colonne binaire par cat√©gorie." },
          { marker: "handle_unknown='ignore'", text: "Si une ville inconnue appara√Æt en production, au lieu de crasher, le mod√®le mettra 0 partout. Indispensable pour la robustesse." },
        ]
      },
      {
        title: "Feature Engineering",
        explain: "L'art de cr√©er des variables plus informatives √† partir des existantes. Un bon feature engineering vaut souvent plus qu'un mod√®le sophistiqu√©.",
        code_block: <<~PY,
          # Prix au m¬≤ (plus informatif que le prix brut)
          df["price_per_m2"] = df["price"] / df["surface"]

          # √Çge de la maison (√† partir de l'ann√©e de construction)
          df["age"] = 2026 - df["year_built"]

          # Variables bool√©ennes combin√©es
          df["has_outdoor"] = ((df["garden"] == "Oui") | (df["pool"] == "Oui")).astype(int)

          # Extraction depuis une date
          df["sale_date"] = pd.to_datetime(df["sale_date"])
          df["sale_month"] = df["sale_date"].dt.month
          df["is_summer"]  = df["sale_month"].isin([6,7,8]).astype(int)

          print(df[["surface", "price", "price_per_m2", "has_outdoor"]].head(3))
        PY
        output: <<~OUT,
             surface   price  price_per_m2  has_outdoor
          0       85  320000       3765           1
          1      120  185000       1542           0
          2       45  142000       3156           1
        OUT
        code_notes: [
          { marker: "price_per_m2", text: "Un appart de 120 m¬≤ √† 185K (1542 ‚Ç¨/m¬≤) est tr√®s diff√©rent d'un 45 m¬≤ √† 142K (3156 ‚Ç¨/m¬≤). Le ratio donne une info que le mod√®le ne peut pas calculer seul." },
          { marker: "is_summer", text: "Les prix immobiliers sont souvent plus √©lev√©s en √©t√©. Transformer un mois en saison binaire aide le mod√®le √† capturer cette saisonnalit√©." },
        ]
      },
      {
        title: "Scaling / Normalisation",
        explain: "Remettre toutes les variables num√©riques √† la m√™me √©chelle. Indispensable pour KNN, SVM, r√©seaux de neurones, r√©gression logistique. Les arbres s'en fichent.",
        code_block: <<~PY,
          from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

          # StandardScaler : moyenne=0, √©cart-type=1
          scaler = StandardScaler()
          X_train_sc = scaler.fit_transform(X_train[["surface", "age", "rooms"]])

          # ‚ö†Ô∏è R√àGLE D'OR : fit sur TRAIN, transform sur TEST
          X_test_sc = scaler.transform(X_test[["surface", "age", "rooms"]])

          print("Avant scaling :")
          print(f"  surface : mean={X_train['surface'].mean():.0f}, std={X_train['surface'].std():.0f}")
          print("Apr√®s scaling :")
          print(f"  surface : mean={X_train_sc[:,0].mean():.2f}, std={X_train_sc[:,0].std():.2f}")
        PY
        output: <<~OUT,
          Avant scaling :
            surface : mean=102, std=48
            age     : mean=22,  std=15
            rooms   : mean=4,   std=2

          Apr√®s scaling :
            surface : mean=0.00, std=1.00
            age     : mean=0.00, std=1.00
            rooms   : mean=0.00, std=1.00
        OUT
        code_notes: [
          { marker: "fit_transform(X_train)", text: "<code>fit</code> calcule la moyenne et l'√©cart-type du train. <code>transform</code> applique (x - mean) / std. Les deux en une ligne." },
          { marker: "transform(X_test)", text: "‚ö†Ô∏è On ne fait PAS <code>fit_transform</code> sur le test ! On r√©utilise la moyenne et std du train. Sinon, on 'triche' en regardant les donn√©es de test." },
          { marker: "StandardScaler", text: "Choix : <code>StandardScaler</code> (normal), <code>MinMaxScaler</code> (born√© [0,1], images), <code>RobustScaler</code> (r√©sistant aux outliers, utilise m√©diane/IQR)." },
        ]
      },
      {
        title: "Train / Test Split",
        explain: "S√©parer en deux : un jeu pour apprendre, un pour √©valuer. Comme s√©parer les questions d'un examen en r√©vision et examen blanc.",
        code_block: <<~PY,
          from sklearn.model_selection import train_test_split

          X = df.drop(columns=["price"])
          y = df["price"]

          X_train, X_test, y_train, y_test = train_test_split(
              X, y,
              test_size=0.3,     # 30% pour le test
              random_state=42    # reproductibilit√©
          )

          print(f"Train : {X_train.shape[0]} lignes ({X_train.shape[0]/len(X)*100:.0f}%)")
          print(f"Test  : {X_test.shape[0]} lignes ({X_test.shape[0]/len(X)*100:.0f}%)")
        PY
        output: <<~OUT,
          Train : 1010 lignes (70%)
          Test  : 433 lignes (30%)
        OUT
        code_notes: [
          { marker: "test_size=0.3", text: "70/30 est standard. Pour un petit dataset (< 1000), on peut faire 80/20." },
          { marker: "random_state=42", text: "Fixe le hasard pour la reproductibilit√©. Sans √ßa, chaque ex√©cution donne un split diff√©rent." },
          { marker: "‚ö†Ô∏è Ordre", text: "TOUJOURS splitter AVANT le scaling. Si tu scales avant, le scaler a vu les donn√©es de test ‚Üí data leakage." },
        ]
      },
    ],
    tips: { title: "‚ö†Ô∏è Pi√®ges courants", items: [
      "Ne JAMAIS fit le scaler sur le test set ‚Üí fit_transform() sur train, transform() sur test",
      "OneHotEncoder peut exploser la dimensionalit√© (100 villes = 100 colonnes)",
      "Les arbres de d√©cision n'ont PAS besoin de scaling",
      "V√©rifier les types : un 'code postal' num√©rique est en r√©alit√© cat√©goriel",
    ]},
    code_filename: "preprocessing_pipeline_complet.py",
    code_content: <<~PY,
      import pandas as pd
      from sklearn.model_selection import train_test_split
      from sklearn.impute import SimpleImputer
      from sklearn.preprocessing import StandardScaler, OneHotEncoder
      from sklearn.compose import ColumnTransformer
      from sklearn.pipeline import Pipeline

      df = pd.read_csv("housing.csv")
      df = df.drop_duplicates()
      X = df.drop(columns=["price"])
      y = df["price"]

      num_cols = X.select_dtypes(include="number").columns.tolist()
      cat_cols = X.select_dtypes(include="object").columns.tolist()

      num_pipe = Pipeline([("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())])
      cat_pipe = Pipeline([("imputer", SimpleImputer(strategy="most_frequent")),
                           ("encoder", OneHotEncoder(handle_unknown="ignore"))])
      preprocessor = ColumnTransformer([("num", num_pipe, num_cols), ("cat", cat_pipe, cat_cols)])

      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
      X_train_processed = preprocessor.fit_transform(X_train)
      X_test_processed  = preprocessor.transform(X_test)
      print(f"Train: {X_train_processed.shape}, Test: {X_test_processed.shape}")
    PY
  },

  # ============================================================
  # EDA
  # ============================================================
  {
    slug: "eda",
    title: "Exploratory Data Analysis",
    icon: "üìä",
    badge: "Analyse",
    badge_color: "cyan",
    subtitle: "Comprendre la structure, les distributions et les relations dans les donn√©es",
    analogy_title: "L'analogie du d√©tective",
    analogy_text: "L'EDA, c'est l'enqu√™te pr√©liminaire avant de r√©soudre l'affaire. Tu ne lances pas de mod√®le √† l'aveugle : tu regardes les indices (distributions), tu cherches des corr√©lations (qui conna√Æt qui ?), tu rep√®res les anomalies. Plus ton enqu√™te est minutieuse, plus ton mod√®le sera pertinent.",
    steps: [
      {
        title: "Vue d'ensemble",
        explain: "Le premier coup d'≈ìil. Combien de lignes et colonnes ? Quels types ? Des valeurs manquantes ?",
        code_block: <<~PY,
          import pandas as pd
          df = pd.read_csv("housing.csv")

          print(f"Shape : {df.shape}")
          print(f"\\nTypes :\\n{df.dtypes}")
          print(f"\\nValeurs manquantes :\\n{df.isnull().sum()}")
          print(f"\\nStats :\\n{df.describe().round(1)}")
        PY
        output: <<~OUT,
          Shape : (1460, 7)

          Types :
          surface     int64
          rooms       int64
          age         int64
          city       object
          price       int64

          Stats :
                  surface  rooms    age      price
          count  1437.0   1460.0  1415.0   1460.0
          mean    102.3      3.8    22.1  245800.0
          min      18.0      1.0     0.0   45000.0
          50%      92.0      3.0    20.0  215000.0
          max     450.0     12.0    95.0 1200000.0
        OUT
        code_notes: [
          { marker: "describe()", text: "Le min/max r√©v√®le les outliers. Ici <code>surface=450</code> et <code>age=95</code> sont suspects. Le <code>50%</code> (m√©diane) donne le centre r√©el." },
        ]
      },
      {
        title: "Distributions (analyse univari√©e)",
        explain: "Chaque variable individuellement. Les prix suivent-ils une gaussienne ? Y a-t-il des pics ? √áa conditionne le scaling et les transformations.",
        code_block: <<~PY,
          import matplotlib.pyplot as plt
          import seaborn as sns
          import numpy as np

          fig, axes = plt.subplots(2, 2, figsize=(12, 8))

          # Histogramme + KDE
          sns.histplot(df["price"], bins=30, kde=True, ax=axes[0,0])
          axes[0,0].set_title("Distribution des prix")
          axes[0,0].axvline(df["price"].median(), color="red", linestyle="--")

          # Boxplot (outliers visibles)
          sns.boxplot(x=df["surface"], ax=axes[0,1])
          axes[0,1].set_title("Boxplot surface")

          # Countplot cat√©gorielle
          sns.countplot(y=df["city"], order=df["city"].value_counts().index, ax=axes[1,0])

          # Log-transform si skewed
          sns.histplot(df["price"].apply(np.log1p), bins=30, ax=axes[1,1])
          axes[1,1].set_title("log(price) ‚Äî plus gaussien")
          plt.tight_layout()
        PY
        output: <<~OUT,
          üìä Distribution des prix :
          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  ‚Üê pic autour de 200K
          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                    ‚Üê longue tra√Æne √† droite
          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
          ‚ñà‚ñà‚ñà
          ‚ñà                               ‚Üê quelques villas > 800K

          ‚Üí Distribution skewed ‚Üí envisager log(price) comme target
        OUT
        code_notes: [
          { marker: "kde=True", text: "Ajoute une courbe liss√©e sur l'histogramme. Permet de voir la forme de la distribution sans d√©pendre du nombre de bins." },
          { marker: "log1p()", text: "Si la distribution est tr√®s asym√©trique, le log la rend plus gaussienne. Beaucoup de mod√®les fonctionnent mieux avec des distributions sym√©triques." },
        ]
      },
      {
        title: "Relations entre variables (bivari√©e)",
        explain: "Surface et prix bougent-ils ensemble ? Les maisons avec jardin sont-elles plus ch√®res ? C'est ici qu'on identifie les features √† fort pouvoir pr√©dictif.",
        code_block: <<~PY,
          fig, axes = plt.subplots(1, 3, figsize=(15, 4))

          # Scatter : surface vs price
          axes[0].scatter(df["surface"], df["price"], alpha=0.3, s=10)
          axes[0].set_xlabel("Surface (m¬≤)")
          axes[0].set_ylabel("Prix (‚Ç¨)")
          axes[0].set_title("Surface vs Prix")

          # Boxplot group√© : prix par ville
          sns.boxplot(data=df, x="city", y="price", ax=axes[1])
          axes[1].set_title("Prix par ville")

          # Pairplot
          sns.pairplot(df[["surface", "rooms", "age", "price"]], corner=True)
        PY
        output: <<~OUT,
          üìä Surface vs Prix :
          Relation quasi-lin√©aire visible.
          surface sera la feature la plus pr√©dictive.
          Paris nettement plus cher que Lyon et Marseille.
        OUT
        code_notes: [
          { marker: "alpha=0.3", text: "Semi-transparence. Avec 1460 points, sans √ßa on voit une bouillie. Avec, on distingue les zones denses." },
          { marker: "pairplot(corner=True)", text: "Tous les scatter plots 2 √† 2 en une commande. <code>corner=True</code> √©vite la redondance." },
        ]
      },
      {
        title: "Matrice de corr√©lation",
        explain: "La carte thermique qui r√©sume toutes les relations num√©riques en un coup d'≈ìil.",
        code_block: <<~PY,
          corr = df.corr(numeric_only=True)

          plt.figure(figsize=(8, 6))
          sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f",
                      vmin=-1, vmax=1, center=0)
          plt.title("Matrice de corr√©lation")

          # Top corr√©lations avec la target
          print(corr["price"].sort_values(ascending=False))
        PY
        output: <<~OUT,
          Corr√©lations avec price :
          surface    0.81  ‚Üê forte !
          rooms      0.68
          age       -0.35  ‚Üê n√©gatif (vieux = moins cher)

          ‚ö†Ô∏è surface & rooms corr√©l√©s √† 0.72 ‚Üí multicolin√©arit√©
          ‚Üí Surveiller en r√©gression lin√©aire (VIF)
        OUT
        code_notes: [
          { marker: "annot=True", text: "Affiche les coefficients directement sur la heatmap." },
          { marker: "corr > 0.8", text: "Deux features corr√©l√©es √† > 0.8 ‚Üí multicolin√©arit√©. En r√©gression lin√©aire, √ßa rend les coefficients instables." },
          { marker: "corr['price']", text: "Les corr√©lations avec la target. <code>surface: 0.81</code> est la feature la plus pr√©dictive." },
        ]
      },
      {
        title: "Analyse des valeurs manquantes",
        explain: "Les donn√©es manquent-elles au hasard ou y a-t-il un pattern ? C'est crucial pour choisir la bonne strat√©gie d'imputation.",
        code_block: <<~PY,
          import missingno as msno  # pip install missingno

          msno.matrix(df)
          plt.savefig("missing_patterns.png")

          # Types de manquance :
          # MCAR ‚Äî Missing Completely At Random (pas de pattern)
          #        ‚Üí safe to drop or impute mean/median
          # MAR  ‚Äî Missing At Random (d√©pend d'une AUTRE variable)
          #        ‚Üí imputer en fonction de cette variable
          # MNAR ‚Äî Missing Not At Random (d√©pend de la variable elle-m√™me)
          #        ‚Üí le plus dangereux
        PY
        output: <<~OUT,
          surface ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  (23 NaN)
          rooms   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  (0 NaN)
          age     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  (45 NaN)
          city    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë  (12 NaN)
          price   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  (0 NaN)

          ‚Üí NaN dispers√©s (MCAR probable) ‚Üí safe pour imputer m√©diane
        OUT
        code_notes: [
          { marker: "msno.matrix()", text: "Visualise les patterns. Si les trous s'alignent entre colonnes, c'est du MAR (d√©pendance entre variables)." },
          { marker: "MNAR", text: "Le cas vicieux : les donn√©es manquent PARCE QUE leur valeur est extr√™me. Ex: les maisons tr√®s ch√®res n'affichent pas leur prix." },
        ]
      },
      {
        title: "Synth√®se et strat√©gie",
        explain: "Le rapport d'enqu√™te final : insights cl√©s et plan de mod√©lisation.",
        code_block: <<~PY,
          # SYNTH√àSE EDA ‚Äî housing.csv
          # =============================
          # 1. Target (price) : skewed ‚Üí tester log(price)
          # 2. Feature #1 : surface (corr=0.81) ‚Äî relation lin√©aire
          # 3. Feature #2 : rooms (corr=0.68) ‚Äî corr√©l√© √† surface
          # 4. Feature #3 : age (corr=-0.35) ‚Äî faible mais utile
          # 5. Cat√©gorielle : city ‚Äî OneHot (4 villes)
          # 6. Outliers : 23 observations surface > 238 m¬≤
          # 7. NaN : < 5% partout ‚Üí imputer median/most_frequent
          #
          # PLAN :
          # - Baseline : LinearRegression
          # - Tester : Ridge/Lasso (multicolin√©arit√©)
          # - Tester : RandomForest (non-lin√©arit√©s)
          # - M√©trique : RMSE (en ‚Ç¨)
        PY
        output: "",
        code_notes: [
          { marker: "synth√®se", text: "Un bon EDA se termine par un plan d'action clair. L'EDA n'est pas une fin en soi ‚Äî c'est le brief pour la mod√©lisation." },
        ]
      },
    ],
    tips: { title: "üì¶ Toolbox visualisation", items: [
      "matplotlib : contr√¥le fin, base de tout",
      "seaborn : statistiques visuelles √©l√©gantes (histplot, heatmap, pairplot)",
      "plotly : interactif, id√©al pour pr√©sentation",
      "ydata-profiling : EDA automatis√© en 1 ligne ‚Äî ProfileReport(df)",
    ]},
    code_filename: "eda_complet.py",
    code_content: <<~PY,
      import pandas as pd
      import seaborn as sns
      import matplotlib.pyplot as plt
      import numpy as np

      df = pd.read_csv("housing.csv")
      print(df.shape, df.dtypes, df.describe(), sep="\\n\\n")
      df.hist(figsize=(14, 10), bins=30); plt.tight_layout(); plt.show()
      sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm", fmt=".2f"); plt.show()
      print(df.corr(numeric_only=True)["price"].sort_values(ascending=False))
    PY
  },
]
