- slug: preprocessing
  title: Data Preprocessing
  icon: "üßπ"
  badge: Pr√©paration
  badge_color: green
  subtitle: Nettoyer et transformer les donn√©es brutes en un dataset exploitable par un mod√®le
  analogy_title: "L'analogie du chef cuisinier"
  analogy_text: "Le preprocessing, c'est comme la mise en place en cuisine. Avant de cuisiner (entra√Æner un mod√®le), il faut laver les l√©gumes (supprimer les doublons), retirer les parties ab√Æm√©es (g√©rer les valeurs manquantes), tout couper √† la m√™me taille (scaling) et s√©parer les ingr√©dients par type (encodage). Si tu sautes cette √©tape, le plat sera rat√©, m√™me avec la meilleure recette."
  steps:
    - title: Charger et inspecter les donn√©es
      code_hint: "pd.read_csv(), df.shape, df.dtypes, df.info(), df.describe()"
      explain: "Imagine que tu re√ßois un carton rempli de fiches. La premi√®re chose √† faire, c'est de compter combien il y en a, v√©rifier qu'elles sont lisibles, et comprendre ce que chaque colonne repr√©sente."
    - title: G√©rer les valeurs manquantes
      code_hint: "df.isnull().sum() ‚Üí dropna ou SimpleImputer(strategy='median')"
      explain: "Des cases vides dans un tableur. Si 5 % de la colonne '√¢ge' est vide, on remplit avec la m√©diane. Si 80 % est vide, on supprime la colonne. Ne pas inventer de l'information, mais combler intelligemment les trous."
    - title: Supprimer les doublons
      code_hint: "df.duplicated().sum() ‚Üí df.drop_duplicates()"
      explain: "Si le m√™me client appara√Æt 3 fois, le mod√®le pensera que ce profil est 3√ó plus fr√©quent. C'est comme compter 3 fois le m√™me bulletin de vote."
    - title: D√©tecter et traiter les outliers
      code_hint: "IQR : [Q1 ‚àí 1.5√óIQR, Q3 + 1.5√óIQR] ou Z-score > 3"
      explain: "Un √©l√®ve qui mesure '17 m√®tres' est clairement une erreur de saisie. Un outlier risque de fausser le mod√®le ‚Äî on le corrige ou on le supprime."
    - title: Encoder les variables cat√©gorielles
      code_hint: "Ordinales ‚Üí OrdinalEncoder | Nominales ‚Üí OneHotEncoder"
      explain: "Un mod√®le ne comprend que les chiffres. 'Paris=1, Lyon=2, Marseille=3' impliquerait un ordre inexistant. Le One-Hot cr√©e une colonne par cat√©gorie pour √©viter ce pi√®ge."
    - title: Feature Engineering
      code_hint: "Ratios, interactions, extraction datetime, PolynomialFeatures"
      explain: "Si tu as la date de naissance, cr√©e une colonne '√¢ge'. 'Prix au m¬≤' = prix/surface est plus informatif que le prix seul. L'art de donner au mod√®le des indices plus faciles √† exploiter."
    - title: Scaling / Normalisation
      code_hint: "StandardScaler (Œº=0, œÉ=1) | MinMaxScaler [0,1] | RobustScaler"
      explain: "Si 'revenu' va de 15K √† 150K et '√¢ge' de 18 √† 80, le mod√®le pensera que le revenu est plus important ‚Äî simplement parce que les nombres sont plus grands. Le scaling remet tout √† la m√™me √©chelle. Indispensable pour KNN, SVM, r√©seaux de neurones. Les arbres s'en fichent."
    - title: Train / Test Split
      code_hint: "train_test_split(X, y, test_size=0.3, random_state=42)"
      explain: "S√©parer les questions d'un examen en deux paquets : un pour s'entra√Æner, un pour l'√©valuation finale. R√®gle d'or : TOUJOURS splitter AVANT le scaling, sinon tu 'triches' en utilisant de l'info du test set."
  tips:
    title: "‚ö†Ô∏è Pi√®ges courants"
    items:
      - "Ne JAMAIS fit le scaler sur le test set ‚Üí fit_transform() sur train, transform() sur test"
      - "OneHotEncoder peut exploser la dimensionalit√© (100 villes = 100 colonnes)"
      - "Les arbres de d√©cision n'ont PAS besoin de scaling"
      - "V√©rifier les types : un 'code postal' num√©rique est en r√©alit√© cat√©goriel"
  code_filename: preprocessing_complet.py
  code_content: |
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline

    # 1. Charger
    df = pd.read_csv("housing.csv")
    print(df.info(), df.describe())

    # 2. Valeurs manquantes
    print(df.isnull().sum())

    # 3. Doublons
    df = df.drop_duplicates()

    # 4. S√©parer features et target
    X = df.drop(columns=["price"])
    y = df["price"]

    # 5. Identifier les types de colonnes
    num_cols = X.select_dtypes(include="number").columns.tolist()
    cat_cols = X.select_dtypes(include="object").columns.tolist()

    # 6. Sous-pipelines
    num_pipe = Pipeline([
        ("imputer", SimpleImputer(strategy="median")),
        ("scaler",  StandardScaler())
    ])
    cat_pipe = Pipeline([
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("encoder", OneHotEncoder(handle_unknown="ignore"))
    ])
    preprocessor = ColumnTransformer([
        ("num", num_pipe, num_cols),
        ("cat", cat_pipe, cat_cols)
    ])

    # 7. Split AVANT le preprocessing
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # 8. Fit sur train, transform sur test
    X_train_processed = preprocessor.fit_transform(X_train)
    X_test_processed  = preprocessor.transform(X_test)
    print(f"Train: {X_train_processed.shape}, Test: {X_test_processed.shape}")

- slug: eda
  title: Exploratory Data Analysis
  icon: "üìä"
  badge: Analyse
  badge_color: cyan
  subtitle: Comprendre la structure, les distributions et les relations dans les donn√©es
  analogy_title: "L'analogie du d√©tective"
  analogy_text: "L'EDA, c'est l'enqu√™te pr√©liminaire avant de r√©soudre l'affaire. Tu ne lances pas de mod√®le √† l'aveugle : tu regardes les indices (distributions), tu cherches des corr√©lations (qui conna√Æt qui ?), tu rep√®res les anomalies (le suspect aux chaussures boueuses). Plus ton enqu√™te est minutieuse, plus ton mod√®le sera pertinent."
  steps:
    - title: Vue d'ensemble
      code_hint: "df.shape, df.dtypes, df.describe(), df.info()"
      explain: "Combien de lignes et colonnes ? Quels types ? Un √¢ge max √† 999 ? C'est le premier coup d'≈ìil qui donne la temp√©rature du dataset."
    - title: Analyse univari√©e
      code_hint: "Histogrammes, boxplots, value_counts()"
      explain: "Tu regardes chaque variable seule. Les prix suivent-ils une cloche ou une longue tra√Æne ? Une cat√©gorie = 90 % des donn√©es ? √áa conditionne tes choix futurs."
    - title: Analyse bivari√©e
      code_hint: "Scatter plots, boxplots group√©s, sns.pairplot()"
      explain: "Les relations entre variables. Surface et prix bougent-ils ensemble ? Les maisons avec piscine sont-elles plus ch√®res ? C'est ici que tu trouves les features √† fort impact."
    - title: Matrice de corr√©lation
      code_hint: "df.corr() + sns.heatmap(annot=True)"
      explain: "La carte thermique des relations. Corr√©lation > 0.8 entre deux features = multicolin√©arit√©, tu peux en supprimer une. Regarde aussi la corr√©lation avec la target."
    - title: Analyse des valeurs manquantes
      code_hint: "msno.matrix(df) ‚Äî Patterns MCAR, MAR, MNAR"
      explain: "Si les revenus manquent uniquement pour les < 18 ans, ce n'est pas al√©atoire. Le type de 'manquance' d√©termine comment les traiter."
    - title: Synth√®se et hypoth√®ses
      code_hint: "Formuler insights et strat√©gie de mod√©lisation"
      explain: "Le 'rapport d'enqu√™te' : les 3-4 features les plus li√©es √† la target, les transformations n√©cessaires, les mod√®les √† tester. Un bon EDA = 50 % du travail."
  tips:
    title: "üì¶ Toolbox visualisation"
    items:
      - "matplotlib : contr√¥le fin, base de tout"
      - "seaborn : statistiques visuelles √©l√©gantes"
      - "plotly : interactif, id√©al pour pr√©sentation"
      - "ydata-profiling : EDA automatis√© en 1 ligne"
  code_filename: eda_complet.py
  code_content: |
    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt

    df = pd.read_csv("housing.csv")

    # 1. Vue d'ensemble
    print(df.shape)
    print(df.describe())

    # 2. Distributions num√©riques
    df.hist(figsize=(14, 10), bins=30)
    plt.tight_layout()
    plt.show()

    # 3. Matrice de corr√©lation
    plt.figure(figsize=(10, 8))
    sns.heatmap(df.corr(numeric_only=True), annot=True, cmap="coolwarm", fmt=".2f")
    plt.title("Matrice de corr√©lation")
    plt.show()

    # 4. Relation feature vs target
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    for i, col in enumerate(["surface", "rooms", "age"]):
        axes[i].scatter(df[col], df["price"], alpha=0.3)
        axes[i].set_xlabel(col)
        axes[i].set_ylabel("price")
    plt.tight_layout()
    plt.show()

    # 5. Top corr√©lations
    print(df.corr(numeric_only=True)["price"].sort_values(ascending=False))

- slug: linreg
  title: R√©gression Lin√©aire
  icon: "üìà"
  badge: Supervis√©
  badge_color: accent
  subtitle: "Pr√©dire une valeur continue ‚Äî y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô"
  analogy_title: "L'analogie du fil tendu"
  analogy_text: "Imagine un nuage de points (surface vs prix). La r√©gression lin√©aire trouve le fil tendu qui passe au plus pr√®s de tous les points. Ce fil te permet de pr√©dire : 'pour 80 m¬≤, le prix devrait √™tre autour de X'. Si les points forment un nuage allong√©, la droite fonctionnera. S'ils forment un nuage rond ou une courbe, il faudra un mod√®le plus complexe."
  steps:
    - title: V√©rifier les hypoth√®ses
      code_hint: "Lin√©arit√©, homosc√©dasticit√©, normalit√© r√©sidus, VIF < 5"
      explain: "La r√©gression a des 'conditions d'utilisation'. Si la relation X-y est une courbe, le mod√®le sera mauvais. C'est comme utiliser une r√®gle pour mesurer un terrain vallonn√© : √ßa marche si le terrain est plat."
    - title: Pr√©parer les features
      code_hint: "StandardScaler pour comparer les coefficients. PolynomialFeatures si courbe."
      explain: "Le scaling n'est pas obligatoire pour fonctionner, mais indispensable pour comparer l'importance des coefficients entre eux."
    - title: Entra√Æner le mod√®le
      code_hint: "LinearRegression().fit(X_train, y_train) ‚Äî Minimise OLS"
      explain: "Le mod√®le cherche la droite qui minimise la distance verticale entre chaque point et la droite, √©lev√©e au carr√©. R√©sultat : des coefficients (pente par feature) et un intercept."
    - title: "R√©gulariser si overfitting"
      code_hint: "Ridge (L2) r√©duit les coefficients | Lasso (L1) les met √† z√©ro"
      explain: "Ridge = petits poids attach√©s aux coefficients pour les emp√™cher de devenir trop grands. Lasso = un filtre qui ne garde que les variables vraiment utiles en mettant les autres √† z√©ro."
    - title: √âvaluer avec cross-validation
      code_hint: "cross_val_score(model, X, y, cv=5, scoring='r2')"
      explain: "On d√©coupe les donn√©es en 5 morceaux, on entra√Æne 5 fois en laissant un morceau de c√¥t√© √† chaque fois. Score plus fiable qu'un seul split."
  tips:
    title: "üìè M√©triques"
    items:
      - "R¬≤ : % de variance expliqu√©e. 0.85 = le mod√®le explique 85 % des variations"
      - "RMSE : erreur moyenne. Si RMSE = 15K‚Ç¨ pour des prix, le mod√®le se trompe de ~15K‚Ç¨"
      - "MAE : erreur absolue moyenne, plus robuste aux outliers"
  code_filename: linear_regression.py
  code_content: |
    import numpy as np
    from sklearn.linear_model import LinearRegression, Ridge
    from sklearn.model_selection import train_test_split, cross_val_score
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import mean_squared_error, r2_score

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

    # Scaling
    scaler = StandardScaler()
    X_train_sc = scaler.fit_transform(X_train)
    X_test_sc  = scaler.transform(X_test)

    # Mod√®le simple
    model = LinearRegression()
    model.fit(X_train_sc, y_train)

    y_pred = model.predict(X_test_sc)
    print(f"R¬≤ train : {model.score(X_train_sc, y_train):.3f}")
    print(f"R¬≤ test  : {r2_score(y_test, y_pred):.3f}")
    print(f"RMSE     : {np.sqrt(mean_squared_error(y_test, y_pred)):.0f}")

    # Cross-validation
    cv = cross_val_score(model, X_train_sc, y_train, cv=5, scoring="r2")
    print(f"CV R¬≤ : {cv.mean():.3f} ¬± {cv.std():.3f}")

    # Si overfitting ‚Üí Ridge
    ridge = Ridge(alpha=1.0)
    ridge.fit(X_train_sc, y_train)
    print(f"Ridge R¬≤ test : {ridge.score(X_test_sc, y_test):.3f}")

- slug: logreg
  title: R√©gression Logistique
  icon: "üéØ"
  badge: Classification
  badge_color: accent
  subtitle: "Pr√©dire une cat√©gorie (oui/non, spam/pas spam) via une probabilit√©"
  analogy_title: "L'analogie du thermom√®tre de confiance"
  analogy_text: "Un m√©decin doit d√©cider si un patient est malade. La r√©gression logistique prend les sympt√¥mes et sort une probabilit√© entre 0 % et 100 %. Au-del√† du seuil (50 % par d√©faut), elle dit 'malade'. Mais tu peux ajuster : si rater un malade est grave, baisse le seuil √† 30 % (plus de faux positifs, moins de malades rat√©s)."
  steps:
    - title: V√©rifier le contexte
      code_hint: "y.value_counts(normalize=True) ‚Äî Classes d√©s√©quilibr√©es ?"
      explain: "Si 95 % de 'pas spam' et 5 % de 'spam', un mod√®le qui dit toujours 'pas spam' a 95 % d'accuracy. Trompeur ! Utiliser class_weight='balanced' et √©valuer avec precision/recall."
    - title: Pr√©parer les donn√©es
      code_hint: "StandardScaler ‚Äî la sigmo√Øde est sensible √† l'√©chelle"
      explain: "La logistique passe les donn√©es par une fonction en S (sigmo√Øde) qui compresse entre 0 et 1. Si une feature a des valeurs 100√ó plus grandes, elle domine la d√©cision."
    - title: Entra√Æner
      code_hint: "LogisticRegression(C=1.0, max_iter=1000)"
      explain: "C contr√¥le le biais/variance. C grand = colle aux donn√©es (overfitting). C petit = plus laxiste mais g√©n√©ralise mieux. Un curseur entre 'apprendre par c≈ìur' et 'comprendre la tendance'."
    - title: Ajuster le seuil de d√©cision
      code_hint: "model.predict_proba(X) ‚Äî seuil par d√©faut 0.5"
      explain: "La d√©cision m√©tier cl√©. Fraude : seuil bas (0.3) pour ne rater personne. Filtre spam : seuil haut (0.7) pour √©viter de bloquer un vrai mail."
    - title: √âvaluer
      code_hint: "confusion_matrix(), classification_report(), ROC + AUC"
      explain: "La matrice de confusion montre les 4 cas (TP, FP, TN, FN). L'AUC r√©sume la performance √† tous les seuils : 1.0 = parfait, 0.5 = pile ou face."
  tips:
    title: "üìè M√©triques de classification"
    items:
      - "Accuracy : % bonnes pr√©dictions ‚Äî trompeuse si classes d√©s√©quilibr√©es"
      - "Precision : parmi les pr√©dits positifs, combien le sont vraiment ?"
      - "Recall : parmi les vrais positifs, combien sont d√©tect√©s ?"
      - "F1-Score : moyenne harmonique precision/recall"
      - "AUC-ROC : ind√©pendant du seuil, 1 = parfait"
  code_filename: logistic_regression.py
  code_content: |
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
    import matplotlib.pyplot as plt

    model = LogisticRegression(C=1.0, max_iter=1000, class_weight="balanced")
    model.fit(X_train_sc, y_train)

    y_pred = model.predict(X_test_sc)
    y_proba = model.predict_proba(X_test_sc)[:, 1]

    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))

    # AUC-ROC
    auc = roc_auc_score(y_test, y_proba)
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    plt.plot(fpr, tpr, label=f"AUC = {auc:.2f}")
    plt.plot([0, 1], [0, 1], "k--")
    plt.xlabel("Faux positifs"); plt.ylabel("Vrais positifs")
    plt.legend(); plt.show()

    # Seuil custom (ex: d√©tection fraude)
    y_pred_03 = (y_proba >= 0.3).astype(int)
    print("Seuil 0.3 :", classification_report(y_test, y_pred_03))

- slug: trees
  title: "Arbres & Random Forest"
  icon: "üå≥"
  badge: Supervis√©
  badge_color: green
  subtitle: "Pr√©dire en posant des questions successives ‚Äî comme un jeu de 'Qui est-ce ?'"
  analogy_title: "L'analogie du jeu 'Qui est-ce ?'"
  analogy_text: "Un arbre de d√©cision pose la question qui √©limine le plus de candidats √† chaque √©tape. 'Prix > 200K ? ‚Üí Oui. Surface > 80m¬≤ ? ‚Üí Non. Jardin ? ‚Üí Oui ‚Üí Quartier r√©sidentiel.' Le Random Forest demande l'avis √† 100 joueurs avec chacun un sous-ensemble diff√©rent de questions, puis prend le vote majoritaire. Plus fiable qu'un seul joueur."
  steps:
    - title: "Avantages des arbres"
      code_hint: "Pas de scaling. G√®rent cat√©gorielles. Interpr√©tables. Non-lin√©aires."
      explain: "Un des rares mod√®les montrables √† un non-technicien : 'si le client a > 3 achats ET un panier > 50 ‚Ç¨, il reviendra √† 80 %'. Pas de scaling car l'arbre fait des comparaisons (> ou ‚â§), pas des calculs de distance."
    - title: "Decision Tree simple"
      code_hint: "DecisionTreeClassifier(max_depth=5, min_samples_leaf=10)"
      explain: "Sans contrainte, l'arbre grandit jusqu'√† une feuille par observation ‚Äî overfitting massif. C'est comme cr√©er une r√®gle pour chaque cas particulier au lieu de comprendre le pattern. max_depth et min_samples_leaf sont les gardes-fous."
    - title: "Random Forest (Bagging)"
      code_hint: "RandomForestClassifier(n_estimators=100, max_features='sqrt')"
      explain: "100 arbres, chacun sur un √©chantillon al√©atoire + sous-ensemble de features. Leur vote collectif est bien plus fiable qu'un arbre seul. C'est la 'sagesse de la foule' appliqu√©e au ML."
    - title: "Feature Importance"
      code_hint: "model.feature_importances_ ‚Üí barplot"
      explain: "Combien chaque feature contribue √† r√©duire l'erreur dans les splits. Attention : deux features corr√©l√©es se partagent l'importance, ce qui peut √™tre trompeur."
  tips:
    title: "üîë Bagging vs Boosting"
    items:
      - "Bagging (Random Forest) : arbres ind√©pendants en parall√®le ‚Üí r√©duit la variance"
      - "Boosting : arbres s√©quentiels qui corrigent les erreurs ‚Üí r√©duit le biais"
      - "RF : moins de risque d'overfitting, plus robuste"
      - "Boosting : souvent plus performant, plus sensible aux hyperparam√®tres"
  code_filename: random_forest.py
  code_content: |
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import cross_val_score, RandomizedSearchCV
    import matplotlib.pyplot as plt
    import numpy as np

    # Random Forest (pas de scaling !)
    rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
    rf.fit(X_train, y_train)
    print(f"Train: {rf.score(X_train, y_train):.3f}, Test: {rf.score(X_test, y_test):.3f}")

    # Cross-validation
    cv = cross_val_score(rf, X_train, y_train, cv=5)
    print(f"CV accuracy : {cv.mean():.3f} ¬± {cv.std():.3f}")

    # Tuning
    params = {
        "n_estimators": [50, 100, 200, 300],
        "max_depth": [5, 10, 20, None],
        "min_samples_leaf": [1, 5, 10],
        "max_features": ["sqrt", "log2"]
    }
    search = RandomizedSearchCV(rf, params, n_iter=20, cv=5, random_state=42)
    search.fit(X_train, y_train)
    print(f"Best params : {search.best_params_}")
    print(f"Best score  : {search.best_score_:.3f}")

    # Feature importance
    importances = search.best_estimator_.feature_importances_
    idx = np.argsort(importances)[-10:]
    plt.barh(range(10), importances[idx])
    plt.yticks(range(10), [feature_names[i] for i in idx])
    plt.title("Top 10 Feature Importances")
    plt.show()

- slug: boosting
  title: "Gradient Boosting / XGBoost"
  icon: "üöÄ"
  badge: Supervis√©
  badge_color: orange
  subtitle: "Des arbres qui apprennent de leurs erreurs, un par un"
  analogy_title: "L'analogie des couches de peinture"
  analogy_text: "Tu peins un portrait. La premi√®re couche est grossi√®re : les grandes formes. La deuxi√®me corrige : les ombres, les proportions. Chaque couche affine un peu plus. Le Gradient Boosting fonctionne pareil : chaque nouvel arbre se concentre sur les erreurs du pr√©c√©dent. 100 petits arbres 'moyens' qui se corrigent donnent un r√©sultat souvent meilleur qu'un seul gros arbre. C'est LE mod√®le roi pour les donn√©es tabulaires."
  steps:
    - title: "Choisir l'impl√©mentation"
      code_hint: "XGBClassifier (rapide) ou LGBMClassifier (tr√®s gros datasets)"
      explain: "Le sklearn GradientBoosting est p√©dagogique mais lent. XGBoost est le go-to de Kaggle. LightGBM est plus rapide sur des millions de lignes gr√¢ce √† sa croissance leaf-wise."
    - title: "Hyperparam√®tres cl√©s"
      code_hint: "learning_rate (0.01‚Äì0.3), n_estimators (100‚Äì1000), max_depth (3‚Äì8)"
      explain: "learning_rate = √† quel point chaque arbre 'ose' corriger. 0.01 = prudent (lent mais fin). 0.3 = agressif (rapide mais risque). R√®gle d'or : r√©duire le learning rate ET augmenter le nombre d'arbres."
    - title: "Early Stopping"
      code_hint: "early_stopping_rounds=50, eval_set=[(X_val, y_val)]"
      explain: "Plut√¥t que deviner le bon nombre d'arbres, on surveille le score sur un validation set et on arr√™te quand il stagne. Comme arr√™ter de cuire un g√¢teau quand il est dor√©, au lieu de deviner les minutes."
    - title: "Optimiser"
      code_hint: "RandomizedSearchCV (large) puis GridSearchCV (affinage)"
      explain: "Avec tant d'hyperparam√®tres, tester toutes les combinaisons prendrait des jours. D'abord un RandomizedSearch qui pioche 50 combinaisons, puis un GridSearch fin autour de la meilleure zone."
  tips:
    title: "üí° Best practices"
    items:
      - "Commencer avec learning_rate=0.1, n_estimators=100, puis affiner"
      - "XGBoost/LightGBM g√®rent les NaN nativement"
      - "Souvent le meilleur mod√®le pour donn√©es tabulaires structur√©es"
  code_filename: xgboost_workflow.py
  code_content: |
    from xgboost import XGBClassifier
    from sklearn.model_selection import train_test_split, RandomizedSearchCV
    from sklearn.metrics import classification_report

    # Split avec validation set pour early stopping
    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2)

    xgb = XGBClassifier(
        n_estimators=500, learning_rate=0.05, max_depth=6,
        subsample=0.8, colsample_bytree=0.8,
        early_stopping_rounds=50, eval_metric="logloss", random_state=42
    )
    xgb.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=20)
    print(f"Best round : {xgb.best_iteration}")
    print(classification_report(y_test, xgb.predict(X_test)))

    # Tuning
    params = {
        "max_depth": [3, 5, 7], "learning_rate": [0.01, 0.05, 0.1],
        "subsample": [0.6, 0.8, 1.0], "n_estimators": [100, 200, 300]
    }
    search = RandomizedSearchCV(XGBClassifier(), params, n_iter=30, cv=5, scoring="f1")
    search.fit(X_train, y_train)
    print(f"Best F1 : {search.best_score_:.3f}")

- slug: knn
  title: K-Nearest Neighbors
  icon: "üîµ"
  badge: Supervis√©
  badge_color: accent
  subtitle: "Pr√©dire en regardant les voisins les plus proches"
  analogy_title: "L'analogie de l'immobilier"
  analogy_text: "Tu veux estimer le prix de ta maison. Tu regardes les 5 maisons les plus similaires vendues r√©cemment et tu fais la moyenne. C'est exactement KNN. La 'similarit√©' est la distance euclidienne. K=5 ‚Üí 5 voisins. K=1 ‚Üí tr√®s sensible au bruit. K=50 ‚Üí trop flou."
  steps:
    - title: "‚ö†Ô∏è Scaling OBLIGATOIRE"
      code_hint: "StandardScaler ‚Äî sinon les features √† grande √©chelle dominent"
      explain: "Si 'revenu' va de 20K √† 200K et '√¢ge' de 18 √† 80, la distance sera quasi enti√®rement d√©termin√©e par le revenu. C'est comme mesurer la distance en comptant km ET cm : les km √©crasent tout."
    - title: "Choisir K"
      code_hint: "K petit = overfitting. K grand = underfitting. K impair pour binaire."
      explain: "K=1 : 'je copie mon voisin le plus proche' ‚Üí si c'est un outlier, catastrophe. K=100 : on lisse tellement que les fronti√®res disparaissent. L'optimum est typiquement entre 3 et 15."
    - title: "Trouver le K optimal"
      code_hint: "Boucle K=1 √† 20, plot train/test score vs K"
      explain: "Le meilleur K est l√† o√π le score test est maximal. C'est le point d'√©quilibre entre trop simple et trop complexe."
  tips:
    title: "‚ö†Ô∏è Limites"
    items:
      - "Lent en pr√©diction (calcule toutes les distances √† chaque fois)"
      - "Mal√©diction de la dimensionalit√© : mauvais en haute dimension"
      - "Pas de vrai 'mod√®le' ‚Äî lazy learner, stocke tout le dataset"
      - "Bon pour petits datasets avec peu de features"
  code_filename: knn_workflow.py
  code_content: |
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.preprocessing import StandardScaler
    import matplotlib.pyplot as plt

    scaler = StandardScaler()
    X_train_sc = scaler.fit_transform(X_train)
    X_test_sc  = scaler.transform(X_test)

    # Trouver le meilleur K
    train_scores, test_scores = [], []
    k_range = range(1, 21)
    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors=k)
        knn.fit(X_train_sc, y_train)
        train_scores.append(knn.score(X_train_sc, y_train))
        test_scores.append(knn.score(X_test_sc, y_test))

    plt.plot(k_range, train_scores, label="Train")
    plt.plot(k_range, test_scores, label="Test")
    plt.xlabel("K"); plt.ylabel("Accuracy"); plt.legend()
    plt.title("KNN ‚Äî Choix de K"); plt.show()

    best_k = k_range[test_scores.index(max(test_scores))]
    print(f"Meilleur K = {best_k}, accuracy = {max(test_scores):.3f}")

- slug: svm
  title: Support Vector Machine
  icon: "üî≤"
  badge: Supervis√©
  badge_color: pink
  subtitle: "Trouver la fronti√®re optimale entre les classes"
  analogy_title: "L'analogie de la route entre deux villages"
  analogy_text: "Deux villages (classes) s√©par√©s par un champ. SVM trace une route (hyperplan) aussi large que possible entre les deux, pour que les maisons les plus proches (vecteurs de support) soient le plus loin possible. Si les villages ne sont pas s√©parables par une route droite ? Le kernel trick projette dans un espace sup√©rieur ‚Äî comme monter en h√©licopt√®re et voir la s√©paration d'en haut."
  steps:
    - title: "‚ö†Ô∏è Scaling OBLIGATOIRE"
      code_hint: "StandardScaler essentiel"
      explain: "SVM tr√®s sensible √† l'√©chelle des features."
    - title: "Choisir le kernel"
      code_hint: "Lin√©aire si s√©parable, RBF sinon"
      explain: "Le kernel lin√©aire est rapide et interpr√©table. Le RBF projette dans un espace sup√©rieur pour les cas non-lin√©aires."
    - title: "Comprendre C et gamma"
      code_hint: "C = tol√©rance erreurs. gamma = port√©e d'influence."
      explain: "C grand = fronti√®re pr√©cise qui colle aux donn√©es (overfitting). C petit = plus douce, tol√®re des erreurs. gamma grand = influence locale (fronti√®re complexe). gamma petit = influence large (fronti√®re lisse)."
    - title: "GridSearch sur C et gamma"
      code_hint: "GridSearchCV sur C=[0.1, 1, 10, 100] et gamma"
      explain: "Les deux param√®tres interagissent fortement ‚Äî il faut tester des combinaisons."
  tips:
    title: "üí° Quand utiliser SVM ?"
    items:
      - "Efficace en haute dimension (n_features > n_samples)"
      - "Bon pour petits/moyens datasets"
      - "Lent sur gros datasets (> 10K) ‚Üí pr√©f√©rer LinearSVC"
  code_filename: svm_workflow.py
  code_content: |
    from sklearn.svm import SVC
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import GridSearchCV
    from sklearn.metrics import classification_report

    scaler = StandardScaler()
    X_train_sc = scaler.fit_transform(X_train)
    X_test_sc  = scaler.transform(X_test)

    # Grid Search
    params = {
        "C": [0.1, 1, 10, 100],
        "gamma": ["scale", "auto", 0.01, 0.1],
        "kernel": ["rbf", "linear"]
    }
    grid = GridSearchCV(SVC(), params, cv=5, scoring="accuracy", n_jobs=-1)
    grid.fit(X_train_sc, y_train)

    print(f"Best params : {grid.best_params_}")
    print(f"Best CV     : {grid.best_score_:.3f}")
    print(classification_report(y_test, grid.predict(X_test_sc)))

- slug: kmeans
  title: K-Means Clustering
  icon: "üîÆ"
  badge: Non-supervis√©
  badge_color: cyan
  subtitle: "Regrouper automatiquement les donn√©es similaires ‚Äî sans √©tiquettes"
  analogy_title: "L'analogie des boules de p√©tanque"
  analogy_text: "Des boules sur un terrain ‚Äî tu veux identifier les groupes naturels. Tu places K 'cochonnets' (centro√Ødes) au hasard. Chaque boule se rattache au cochonnet le plus proche. Puis tu d√©places chaque cochonnet au centre de ses boules. Et tu recommences. Au bout de quelques it√©rations, les groupes sont stables. La seule question : combien de cochonnets ?"
  steps:
    - title: "Scaling OBLIGATOIRE"
      code_hint: "StandardScaler ‚Äî distance euclidienne"
      explain: "K-Means utilise la distance euclidienne ‚Äî le scaling est indispensable."
    - title: "Choisir K : coude + silhouette"
      code_hint: "Plot inertia vs K (coude) et silhouette_score vs K (max)"
      explain: "L'inertia diminue toujours quand K augmente. Le 'coude' est o√π ajouter un cluster n'am√©liore plus beaucoup. Le silhouette score mesure si chaque point est bien dans son cluster (-1 √† 1)."
    - title: "Analyser les clusters"
      code_hint: "df.groupby('cluster').mean() ‚Äî Nommer les clusters"
      explain: "L'algo donne des num√©ros. C'est √† toi de comprendre : 'cluster 0 = revenu √©lev√© + peu d'achats = riches inactifs'. C'est l√† que le data scientist apporte la valeur business."
  tips:
    title: "‚ö†Ô∏è Limites"
    items:
      - "Assume des clusters sph√©riques de taille similaire"
      - "Sensible √† l'initialisation ‚Üí n_init=10 par d√©faut"
      - "Formes complexes ‚Üí DBSCAN (pas besoin de K)"
      - "Hi√©rarchie ‚Üí AgglomerativeClustering + dendrogramme"
  code_filename: kmeans_workflow.py
  code_content: |
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import silhouette_score
    import matplotlib.pyplot as plt

    X_sc = StandardScaler().fit_transform(X)

    # Coude + silhouette
    inertias, silhouettes = [], []
    for k in range(2, 11):
        km = KMeans(n_clusters=k, n_init=10, random_state=42)
        labels = km.fit_predict(X_sc)
        inertias.append(km.inertia_)
        silhouettes.append(silhouette_score(X_sc, labels))

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    ax1.plot(range(2,11), inertias, "bo-"); ax1.set_title("M√©thode du coude")
    ax2.plot(range(2,11), silhouettes, "ro-"); ax2.set_title("Silhouette Score")
    plt.show()

    # Mod√®le final
    km = KMeans(n_clusters=4, n_init=10, random_state=42)
    df["cluster"] = km.fit_predict(X_sc)
    print(df.groupby("cluster").mean(numeric_only=True).round(2))

- slug: pca
  title: "PCA ‚Äî Composantes Principales"
  icon: "üé≠"
  badge: Non-supervis√©
  badge_color: cyan
  subtitle: "R√©duire les dimensions tout en gardant l'essentiel de l'information"
  analogy_title: "L'analogie de la photo de profil"
  analogy_text: "Imagine une statue 3D que tu veux montrer en 2D. Si tu la photographies de face, tu perds le profil. De 3/4, tu captures l'essentiel des deux angles. La PCA fait pareil : parmi toutes les directions de tes donn√©es, elle trouve celles qui capturent le maximum de variation. 50 features ‚Üí peut-√™tre 5 composantes suffisent pour 95 % de l'info."
  steps:
    - title: "‚ö†Ô∏è Scaling OBLIGATOIRE"
      code_hint: "StandardScaler ‚Äî PCA maximise la variance"
      explain: "Sans scaling, les features √† grande √©chelle dominent les composantes principales."
    - title: "Appliquer la PCA"
      code_hint: "PCA(n_components=0.95) ou PCA(n_components=2)"
      explain: "n_components=0.95 garde assez de composantes pour 95 % de la variance. Comme compresser une photo JPEG : tu perds un peu de d√©tail mais l'image reste reconnaissable."
    - title: "Interpr√©ter"
      code_hint: "pca.explained_variance_ratio_ et pca.components_"
      explain: "Chaque composante est un 'cocktail' de features originales. La premi√®re explique le plus de variance, la deuxi√®me ce qui reste, etc."
  tips:
    title: "üí° Cas d'usage"
    items:
      - "R√©duire la dimensionalit√© avant ML (acc√©l√®re, r√©duit overfitting)"
      - "Visualisation de donn√©es haute dimension en 2D"
      - "R√©duire la multicolin√©arit√©"
      - "Compression d'images"
  code_filename: pca_workflow.py
  code_content: |
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    import matplotlib.pyplot as plt
    import numpy as np

    X_sc = StandardScaler().fit_transform(X)

    # Variance cumul√©e
    pca_full = PCA().fit(X_sc)
    cumsum = np.cumsum(pca_full.explained_variance_ratio_)
    plt.plot(cumsum, "bo-")
    plt.axhline(y=0.95, color="r", linestyle="--", label="95 %")
    plt.xlabel("Composantes"); plt.ylabel("Variance cumul√©e")
    plt.legend(); plt.show()

    # R√©duction (95 % de variance)
    pca = PCA(n_components=0.95)
    X_pca = pca.fit_transform(X_sc)
    print(f"{X_sc.shape[1]} features ‚Üí {X_pca.shape[1]} composantes")

    # Visualisation 2D
    X_2d = PCA(n_components=2).fit_transform(X_sc)
    plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap="viridis", alpha=0.5)
    plt.xlabel("PC1"); plt.ylabel("PC2"); plt.colorbar(); plt.show()

- slug: nn
  title: "Neural Networks (Dense)"
  icon: "üß†"
  badge: Deep Learning
  badge_color: pink
  subtitle: "Des couches de neurones qui apprennent des repr√©sentations complexes"
  analogy_title: "L'analogie de l'entreprise"
  analogy_text: "Un r√©seau de neurones, c'est une entreprise √† plusieurs √©tages. Les donn√©es entrent √† l'accueil (input). Le 1er √©tage d√©tecte des patterns simples : 'client jeune', 'ach√®te le weekend'. Le 2e combine : 'jeune + weekend = √©tudiant'. Le dernier prend la d√©cision. Chaque employ√© (neurone) a un poids (importance). L'entra√Ænement (backpropagation) ajuste les poids pour faire moins d'erreurs."
  steps:
    - title: "Pr√©parer les donn√©es"
      code_hint: "Scaling obligatoire. One-hot encode y si multi-classes."
      explain: "Les poids sont initialis√©s petits. Si les features sont dans des √©chelles tr√®s diff√©rentes, certains neurones saturent et le r√©seau n'apprend pas."
    - title: "Construire l'architecture"
      code_hint: "Dense(64, 'relu') ‚Üí Dense(32, 'relu') ‚Üí Dense(1, 'sigmoid')"
      explain: "ReLU : si input n√©gatif, output = 0, sinon output = input. Simple et efficace. Derni√®re couche : sigmoid (binaire), softmax (multi-classes), rien (r√©gression)."
    - title: "Compiler : optimizer + loss"
      code_hint: "adam + binary_crossentropy ou categorical_crossentropy"
      explain: "Adam adapte le learning rate automatiquement. La cross-entropy p√©nalise fortement les pr√©dictions 'confiantes mais fausses'."
    - title: "Early Stopping"
      code_hint: "EarlyStopping(patience=5, restore_best_weights=True)"
      explain: "Sans √ßa, le r√©seau finit par apprendre par c≈ìur. Le callback arr√™te quand la val_loss stagne et restaure les meilleurs poids trouv√©s."
    - title: "R√©gulariser si overfitting"
      code_hint: "Dropout(0.3), BatchNormalization()"
      explain: "Dropout : on '√©teint' 30 % des neurones al√©atoirement √† chaque batch. Comme interdire aux bons √©l√®ves de parler pour forcer les autres √† participer."
  tips:
    title: "üèóÔ∏è Architecture type"
    items:
      - "R√©gression : Dense(1) sans activation, loss=mse"
      - "Classif binaire : Dense(1, sigmoid), loss=binary_crossentropy"
      - "Multi-classes : Dense(n, softmax), loss=categorical_crossentropy"
  code_filename: neural_network.py
  code_content: |
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
    from tensorflow.keras.callbacks import EarlyStopping
    import matplotlib.pyplot as plt

    model = Sequential([
        Dense(64, activation="relu", input_shape=(X_train_sc.shape[1],)),
        BatchNormalization(),
        Dropout(0.3),
        Dense(32, activation="relu"),
        Dropout(0.2),
        Dense(1, activation="sigmoid")
    ])

    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

    es = EarlyStopping(patience=5, restore_best_weights=True)
    history = model.fit(X_train_sc, y_train, epochs=100, batch_size=32,
                        validation_split=0.2, callbacks=[es])

    # Courbes d'apprentissage
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
    ax1.plot(history.history["loss"], label="Train")
    ax1.plot(history.history["val_loss"], label="Val"); ax1.legend()
    ax2.plot(history.history["accuracy"], label="Train")
    ax2.plot(history.history["val_accuracy"], label="Val"); ax2.legend()
    plt.show()

    print(f"Test accuracy : {model.evaluate(X_test_sc, y_test)[1]:.3f}")

- slug: cnn
  title: "CNN ‚Äî R√©seaux Convolutifs"
  icon: "üñºÔ∏è"
  badge: Deep Learning
  badge_color: pink
  subtitle: "Analyser des images par d√©tection de motifs locaux"
  analogy_title: "L'analogie du chercheur de d√©tails"
  analogy_text: "Quelqu'un regarde une photo avec une loupe. La 1re couche d√©tecte des d√©tails simples : bords, couleurs. La suivante combine : 'bord + courbe = contour de visage'. Les couches profondes reconnaissent des concepts : 'c'est un chat'. Le filtre (kernel) est la loupe qui glisse sur l'image. Le pooling r√©duit la taille en gardant l'essentiel."
  steps:
    - title: "Pr√©parer les images"
      code_hint: "Resize, normaliser (√∑255), data augmentation"
      explain: "Toutes la m√™me taille (150√ó150). Division par 255 pour passer de [0,255] √† [0,1]. Data augmentation = variations artificielles (rotation, flip, zoom) pour augmenter le dataset et r√©duire l'overfitting."
    - title: "Architecture type"
      code_hint: "Conv2D ‚Üí MaxPooling ‚Üí ... ‚Üí Flatten ‚Üí Dense ‚Üí output"
      explain: "Chaque bloc Conv+Pool extrait des features de plus en plus abstraites. Le Flatten transforme la carte 2D en vecteur 1D pour les couches Dense. Filtres : 32 ‚Üí 64 ‚Üí 128."
    - title: "Transfer Learning (peu de donn√©es)"
      code_hint: "VGG16(weights='imagenet', include_top=False) + Dense custom"
      explain: "Un mod√®le pr√©-entra√Æn√© sur 1.4M d'images a d√©j√† appris bords, textures, formes. Tu greffes tes propres couches au sommet. Comme embaucher un expert et lui apprendre la sp√©cificit√© de ton domaine."
  tips:
    title: "üèóÔ∏è Architectures pr√©-entra√Æn√©es"
    items:
      - "VGG16/19 : simple, bon pour le transfer learning p√©dagogique"
      - "ResNet : skip connections, plus profond"
      - "MobileNet : l√©ger, id√©al pour mobile"
      - "EfficientNet : excellent ratio performance/taille"
  code_filename: cnn_transfer_learning.py
  code_content: |
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
    from tensorflow.keras.applications import VGG16
    from tensorflow.keras.callbacks import EarlyStopping
    from tensorflow.keras.preprocessing.image import ImageDataGenerator

    # Transfer Learning (recommand√© si peu de donn√©es)
    base = VGG16(weights="imagenet", include_top=False, input_shape=(150,150,3))
    base.trainable = False  # geler les couches pr√©-entra√Æn√©es

    model = Sequential([
        base,
        Flatten(),
        Dense(256, activation="relu"),
        Dropout(0.5),
        Dense(1, activation="sigmoid")
    ])

    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

    # Data augmentation
    datagen = ImageDataGenerator(
        rescale=1./255, rotation_range=20,
        horizontal_flip=True, zoom_range=0.2, validation_split=0.2
    )
    train_gen = datagen.flow_from_directory(
        "data/train", target_size=(150,150),
        batch_size=32, class_mode="binary", subset="training"
    )

    es = EarlyStopping(patience=5, restore_best_weights=True)
    model.fit(train_gen, epochs=30, callbacks=[es])

- slug: rnn
  title: "RNN / NLP"
  icon: "üìù"
  badge: Deep Learning
  badge_color: pink
  subtitle: "Traiter du texte et des s√©quences en 'lisant' mot par mot"
  analogy_title: "L'analogie du lecteur attentif"
  analogy_text: "Un RNN lit mot par mot avec une 'm√©moire de travail'. Quand il lit 'le chat a mang√© la‚Ä¶', sa m√©moire aide √† pr√©dire 'souris'. Mais le RNN simple oublie vite (comme lire un roman sans notes). Le LSTM r√©sout √ßa avec un carnet de notes (cell state) et des 'portes' : quoi retenir, quoi oublier, quoi noter."
  steps:
    - title: "Preprocessing NLP"
      code_hint: "Nettoyage ‚Üí Tokenizer(num_words=10000) ‚Üí pad_sequences(maxlen=200)"
      explain: "Nettoyage : lowercase, supprimer ponctuation/stopwords. Tokenization : chaque mot = un nombre. Padding : compl√©ter les phrases courtes avec des 0 pour uniformiser la longueur."
    - title: "Embedding"
      code_hint: "Embedding(vocab_size, 128) ‚Äî vecteurs denses pour chaque mot"
      explain: "Le nombre 42 ne dit rien au mod√®le. L'embedding le transforme en un vecteur de 128 dimensions capturant le 'sens'. Des mots similaires (roi/reine) auront des vecteurs proches."
    - title: "LSTM / GRU"
      code_hint: "Bidirectional(LSTM(64)) ‚Äî lit dans les deux sens"
      explain: "Le Bidirectional lit √† l'endroit ET √† l'envers. 'La banque au bord de la rivi√®re' vs 'je vais √† la banque retirer de l'argent' ‚Üí le contexte des deux c√¥t√©s d√©sambigu√Øse."
    - title: "Alternative : Transformers"
      code_hint: "HuggingFace transformers ‚Äî BERT fine-tuning"
      explain: "Les Transformers regardent TOUS les mots en m√™me temps (attention). BERT pr√©-entra√Æn√© + fine-tuning surpasse souvent un LSTM from scratch."
  tips:
    title: "üîë Choix du mod√®le NLP"
    items:
      - "Simple RNN : s√©quences courtes, vanishing gradient sur les longues"
      - "LSTM : d√©pendances longues, le standard"
      - "GRU : version simplifi√©e du LSTM, plus rapide"
      - "Transformers : √©tat de l'art, mais besoin de plus de donn√©es/compute"
  code_filename: rnn_sentiment.py
  code_content: |
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout
    from tensorflow.keras.preprocessing.text import Tokenizer
    from tensorflow.keras.preprocessing.sequence import pad_sequences
    from tensorflow.keras.callbacks import EarlyStopping

    VOCAB_SIZE = 10000
    MAX_LEN = 200

    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token="<OOV>")
    tokenizer.fit_on_texts(X_train_text)
    X_train_pad = pad_sequences(tokenizer.texts_to_sequences(X_train_text), maxlen=MAX_LEN)
    X_test_pad  = pad_sequences(tokenizer.texts_to_sequences(X_test_text), maxlen=MAX_LEN)

    model = Sequential([
        Embedding(VOCAB_SIZE, 128, input_length=MAX_LEN),
        Bidirectional(LSTM(64)),
        Dropout(0.3),
        Dense(32, activation="relu"),
        Dense(1, activation="sigmoid")
    ])

    model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
    es = EarlyStopping(patience=3, restore_best_weights=True)
    model.fit(X_train_pad, y_train, epochs=20, batch_size=64,
              validation_split=0.2, callbacks=[es])

    # Pr√©dire un avis
    test = ["Ce film est absolument magnifique"]
    pad = pad_sequences(tokenizer.texts_to_sequences(test), maxlen=MAX_LEN)
    print(f"Sentiment : {'positif' if model.predict(pad)[0][0] > 0.5 else 'n√©gatif'}")

- slug: pipeline
  title: "Pipeline Sklearn"
  icon: "üîß"
  badge: Architecture
  badge_color: green
  subtitle: "Encha√Æner preprocessing et mod√®le dans un objet unique et reproductible"
  analogy_title: "L'analogie de la cha√Æne de montage"
  analogy_text: "Sans pipeline, tu fais chaque √©tape √† la main avec le risque d'oublier un fit_transform sur le test set. Une Pipeline, c'est une cha√Æne de montage : les donn√©es entrent, passent chaque poste dans l'ordre, et la pr√©diction sort. En une ligne de pipe.fit(X_train, y_train), tout est g√©r√© proprement ‚Äî z√©ro data leakage."
  steps:
    - title: "Identifier les types de colonnes"
      code_hint: "num_cols = X.select_dtypes('number'), cat_cols = X.select_dtypes('object')"
      explain: "Num√©riques ‚Üí imputation m√©diane + scaling. Cat√©gorielles ‚Üí imputation mode + encodage. Le ColumnTransformer applique le bon traitement selon le type."
    - title: "Assembler la pipeline"
      code_hint: "Pipeline([('preprocessing', preprocessor), ('model', RandomForest())])"
      explain: "Un seul objet qui fait TOUT. pipe.fit() applique fit_transform aux √©tapes de preprocessing et fit au mod√®le. pipe.predict() applique transform (pas fit_transform !) puis predict."
    - title: "GridSearch sur la pipeline"
      code_hint: "GridSearchCV(pipe, {'model__max_depth': [3,5,10]})"
      explain: "La notation double underscore model__max_depth acc√®de au param√®tre du mod√®le dans le pipe. Tu optimises preprocessing ET mod√®le simultan√©ment."
  tips:
    title: "üîë Avantages"
    items:
      - "√âlimine le data leakage automatiquement"
      - "Code reproductible et propre"
      - "S√©rialisable : joblib.dump(pipe, 'model.pkl')"
      - "Compatible GridSearchCV pour tout optimiser"
  code_filename: sklearn_pipeline.py
  code_content: |
    from sklearn.pipeline import Pipeline
    from sklearn.compose import ColumnTransformer
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import StandardScaler, OneHotEncoder
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import cross_val_score, GridSearchCV
    import joblib

    num_cols = ["age", "income", "purchase_count"]
    cat_cols = ["city", "gender"]

    num_pipe = Pipeline([("imputer", SimpleImputer(strategy="median")), ("scaler", StandardScaler())])
    cat_pipe = Pipeline([("imputer", SimpleImputer(strategy="most_frequent")),
                         ("encoder", OneHotEncoder(handle_unknown="ignore"))])

    preprocessor = ColumnTransformer([("num", num_pipe, num_cols), ("cat", cat_pipe, cat_cols)])

    pipe = Pipeline([("preprocessing", preprocessor), ("model", RandomForestClassifier(random_state=42))])

    # Cross-validation
    cv = cross_val_score(pipe, X_train, y_train, cv=5, scoring="accuracy")
    print(f"CV : {cv.mean():.3f} ¬± {cv.std():.3f}")

    # GridSearch (double underscore !)
    params = {"model__n_estimators": [50, 100, 200], "model__max_depth": [5, 10, None]}
    grid = GridSearchCV(pipe, params, cv=5, scoring="accuracy", n_jobs=-1)
    grid.fit(X_train, y_train)
    print(f"Best : {grid.best_score_:.3f}, Test : {grid.score(X_test, y_test):.3f}")

    # Sauvegarder (preprocessing + mod√®le inclus)
    joblib.dump(grid.best_estimator_, "pipeline.joblib")

- slug: mlops
  title: "MLOps ‚Äî D√©ploiement"
  icon: "‚òÅÔ∏è"
  badge: Production
  badge_color: orange
  subtitle: "Du notebook Jupyter au mod√®le en production accessible par API"
  analogy_title: "L'analogie du restaurant"
  analogy_text: "Ton notebook, c'est la recette test√©e chez toi. Le d√©ploiement, c'est ouvrir un restaurant. Il faut : emballer la recette (sauvegarder le mod√®le), cr√©er le service en salle (API FastAPI), construire la cuisine pro (Docker), installer dans un lieu accessible (Cloud), mettre une enseigne (Streamlit). Et tout doit tourner m√™me quand tu n'es pas l√† (CI/CD)."
  steps:
    - title: "Sauvegarder le mod√®le"
      code_hint: "joblib.dump(pipe, 'model.joblib') ou model.save('model.h5')"
      explain: "S√©rialiser le pipeline complet (preprocessing + mod√®le). Attention √† sauvegarder TOUT le pipeline, pas juste le mod√®le."
    - title: "API avec FastAPI"
      code_hint: "@app.get('/predict') ‚Üí JSON in, pr√©diction out"
      explain: "L'interface entre le monde ext√©rieur et ton mod√®le. Un front, une app mobile, ou un autre service envoie des donn√©es via HTTP et re√ßoit la pr√©diction."
    - title: "Dockeriser"
      code_hint: "Dockerfile ‚Üí image Python + d√©pendances + code + mod√®le"
      explain: "Docker cr√©e une 'bo√Æte' herm√©tique avec ton environnement exact. Plus de '√ßa marche sur ma machine mais pas sur le serveur'."
    - title: "D√©ployer sur le Cloud"
      code_hint: "GCP Cloud Run ‚Äî serverless, auto-scaling"
      explain: "Push ton image Docker, Google g√®re le reste. 0 utilisateurs = 0 ‚Ç¨. 1000 requ√™tes simultan√©es = auto-scaling."
    - title: "Front avec Streamlit"
      code_hint: "Interface web rapide qui appelle l'API"
      explain: "Streamlit transforme un script Python en interface web interactive en quelques lignes. Parfait pour une d√©mo ou un MVP."
  tips:
    title: "üìã Checklist d√©ploiement"
    items:
      - "Makefile avec commandes standardis√©es (install, test, deploy)"
      - "requirements.txt avec versions fig√©es"
      - ".env pour les variables (jamais dans le code !)"
      - "Tester l'API en local avant de d√©ployer"
      - "Mod√®le sur GCS plut√¥t que dans l'image Docker"
  code_filename: mlops_deploy.py
  code_content: |
    # ===== api.py (FastAPI) =====
    from fastapi import FastAPI
    import joblib
    import pandas as pd

    app = FastAPI()
    pipe = joblib.load("pipeline.joblib")

    @app.get("/")
    def root():
        return {"status": "API is running"}

    @app.get("/predict")
    def predict(age: int, income: float, city: str):
        X = pd.DataFrame([{"age": age, "income": income, "city": city}])
        pred = pipe.predict(X)[0]
        proba = pipe.predict_proba(X)[0].max()
        return {"prediction": int(pred), "confidence": float(proba)}

    # Lancer : uvicorn api:app --reload

    # ===== Dockerfile =====
    # FROM python:3.10-slim
    # WORKDIR /app
    # COPY requirements.txt .
    # RUN pip install -r requirements.txt
    # COPY . .
    # CMD uvicorn api:app --host 0.0.0.0 --port $PORT

    # ===== Appeler l'API =====
    import requests
    r = requests.get("https://my-api.run.app/predict",
                     params={"age": 35, "income": 55000, "city": "Nancy"})
    print(r.json())  # {'prediction': 1, 'confidence': 0.87}
